# Week 18

## TODO
- Reformat weekly log so week number is at the beginning of the filepath for faster tab-ing 
- Get image regression working 
- export a set of test images
  - My House
  - Brunel 
- Expand test dataset to other regions
  - Countryside somewhere 
  - Places with varying ground types and air qualities. There's probably some trad data science to do here. 
- Draft powerpoint lecture on this FYP. 

## Monday
- Researched image regression 
- Despair
- Exported more of test set

## Tuesday 
- More image regression research. 
- Planned week's work
- Tested dataset as a classifier for whether the network can predict whether the SO2 concentration is under 0.004, and it got an error rate of 8% out of the box! 
  - After further work, this is likely untrue. 
- Thought about extending the feature set 
  - Currently it's likely to just be checking whether the result is green or not. the more green can be seen, the higher the SO2 concentration. A larger, more diverse dataset would provide a much more detailed interpolation of roads, etc.
  - A better interpolation would also include nearby square kilometers, due to atmospheric drift.
- Experimented with different resnet architectures. Too big takes too long to train, too small is useless. 
- Experimented with categorical classification methods but realistically more data, a better drop weight plan (meaning more study) and getting the image net to label the intended outputs correctly is more likely to work more effectively.

## Wednesday
- Optimised import pipeline
- put f-strings in basically everything, because they're lovely. 
- A lot of reading about fastai, theory, and the DataBlock API. 
  - It seems I can't just demand fastai outputs the way I want - I have to give it the right input, let it infer the correct type from that, then get the outputs I want. In other news, heavy python use apparently causes brain damage. 
- Updated kanban backlog 

## Thursday
- Experimented further with regression model 
- tested new metrics 
- exported more data. 

## Friday
- Tested different resnet architectures
  - longer net => longer training. This is bad for iteration, which is what I need right now. 
  - At the minute, my rmse isn't even that much better when using a large network. I guess training strategy is more important, using a large net is brute-forcing a problem by throwing computational power at it, rather than solving the problem intelligently. 
  - RMSE error metric isn't perfect, and is subject to outliers. 
  - resnet101 performed well, recognising areas of green as well as urban areas. One particularly interesting result came when it predicted an impossibly high value for some grey, empty fields. This implies that the network recognises not only colour, but the size and orientation of shapes. The fields are large, rectangular, and grey, impying to a network that they may be enormous factories, rather than barren fields, resulting in a much higher prediction than is possible, because warehouses this large are rare, and are unlikely to be in my data set at this point. This theory needs work.
  - Effectively, the longer the resnet, the better its predictions and the more sophisticated its errors will be. When extracting a more detailed feature set, counteracting these errors should be easy. 
    - Theory: the larger networks base decisions on more sophisticated assumptions. It's easier to correct a faulty assumption than it is to correct a range of faulty conclusions, because we're going slightly higher in the knowledge tree. 
- Wrote notes on current training.  
- Started testing multiple greenhouse gases, in the hope that a dual output network would cause conclusions to cross-pollinate - an emissions source is more readily recognised if it emits both CH4 and SO2, in theory. 
