# Week 14

- found fast.ai guidelines on implementing a basic unet. 
- Re-examined transfer learning strategy
  - the cyclical recog/interpolation strategy is irrelevant for this stage in the project. Implement a unet that can infer satellite imagery first, then apply an incremental training approach or a multi-stage inference approach to improve accuracy. Compute speed is relatively irrelevant compared to accuracy. 
  - implement a satellite inference model, then a ghg inference model. 
- background research on google earth engine so I can make better use of its features to expedite development. Of particular interest are the neural preprocessing layers such as preconvolution, but simply getting images out of the model is hard enough.
- Accepted I'm going to have to use google earth engine's online IDE for data ingestion from this horrid, horrid dataset. 
- Accepted the crushing reality I'm going to have to write JavaScript 
- new development plan for next week - move data ingestion to google earth engine's javascript ide, back up what I need/can, then upload that to drive as I need. I can also massively increase the google drive space for a pittance, should I need up to 2TB. I should not need up to 2TB, and if I do something has gone horribly wrong. 
- purchased google colab pro

### Speculation / Notes
- how best to store/serialise models? 
- Can convolution be carried out as preprocessing in the cloud to minimise bandwidth?
  - no - the convolution layers are all necessary for the u-net. Nice idea though. 
- what happens if I incorporate other models as transferred learners - can the land usage classification algorithm improve inference? Can other ghg datasets improve inference? What about human development?
