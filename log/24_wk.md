# Week 24
3 weeks remaining. 

## TODO
- Finalise CNN model as far as possible
  - Complete testing on different resnet depths
  - Test discriminative learning rates
  - Isolate largest failures and use them as a second stage of training, with the test set being the rest of the network to ensure it generalises well. 
- Build basic tabular framework if possible.

## Monday
- Removed custom RMSE implementation because it's not worth the hassle. Average is fine, we can test them properly later. 
- Exported resnet101 implementation
- imported 3000 new images
- Tested variable learning rates
- Experimented with fit\_one\_cycle

## Tuesday
- More testing on variable learning rates:
  - They seem to mostly only work when training an existing network
- Isolated network to find best implementation
  - ERROR STATS HAVE BEEN AGGREGATED THIS ENTIRE TIME!?!

## Wednesday
- Finished importing London images.
- Implemented some basic data management methods
- got feedback from Allan
- planned testing strategy

## Thursday
- Fixed terrible stat predictions. Turns out these networks aren't inexplicably brilliant. 
- Experiments in overfitting with the full dataset 
  - It turns out fast.ai is actually designed to not work well with huge datasets, and trades memory efficiency for speed. Commendable, but rubbish if you're stuck on a cheap GPU.  
- Exported a CSV of my best predictions

## Friday
- Wrote section 7 

## Saturday
- More stats analysis

## Sunday
- Looked back at what worked and what didn't. Why haven't I been doing this the whole time?
- Trained a net that works at 11%
- Exported lots of test data 
 
