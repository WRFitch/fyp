# Week 17 

# TODO 
- Meet with Allan and talk about strategies to catch up with where I need to be 
- Develop inference model 
  - Complete inference of one dataset - start with something easy and high-detail, like NO2 or SO2.
  - Sort file storage issues once and for all. If you have to pay for 2TB storage it's only Â£8/mo, and precaching necessary data would be far easier than 
- Create write-up draft skeleton 
  - Armature strategy - build an armature, then layer on muscles, skin, and expressivity. Fine-tune previous stage while working on the current stage. 
- Write all project criticisms into one document for import into the write-up
  - What benefits do unproven inferences provide? 
- Cancel Paperspace subscription. 

#### Monday
- Updated to use GeoTIFF because it has better compression and fewer steps in the import chain. I also don't have to import tensorflow, which is presumably very heavyweight, just to process some images into normal images.
- Realised transfer learning isn't likely to work - it's going to upscale the image to match satellite imagery, which isn't what we want

#### Tuesday
- Implemented GeoTIFF -> PNG conversion. 
  - google drive's slow CRUD is very frustrating. 
  - Earth engine's mismatched RGB bands is also frustrating
- Planned out tabular learner architecture
  - Build a basic inferral network that can take a 100px by 100px image as an input (1km square on a side) and the ghg data as outputs. Train this network as best you can, but accept it's not going to work perfectly. 
  - Use a layer of this network (defined by experimentation and research) as inputs for a tabular recommender which includes the other greenhouse gas data. 
    - could a few convolution masks do the trick? 
  - The first component of this double network can be used to infer ghg concentrations from a given picture, the second uses this as a generated component for further analysis and more detailed inference. 
  - Incidentally, tabular analysis also invites further comparison. What other factors can be used in this tabular analysis? NB as interesting as this is, **do not focus on this over getting the project complete.**
    - Land Prices
    - prevailing wind direction 
    - land use
    - population density
    - land height 
    - biodiversity
    - Atmospheric water vapour 
- Refactored image pipeline for brevity
- imported ghg datasets as CSV 

#### Wednesday 
- Cleaned up import pipeline script
- Started on tabular recommender script so I can figure out what I actually need from this dataset.
  - set up fastai script 
  - ran through cat tutorial - hooray, finally doing ai work!
- Started writing dissertation in earnest. 
  - Got the basics of an Abstract in
  - copied in project synopsis from old assignment.

#### Thursday
- Tested image import
  - .tif worked fine this whole time!? If this is accidental fine, but if there's undocumented tif support I'm not happy.
  - Never mind fastai doesn't understand tif files
- Images can be imported into a fastai dataloader 
- Fastai can access data from csv, meaning all datasets can be combined into one csv with the following data: 
  - A column for each GHG data point 
  - A filepath for an imagefile with each square km at 100px/km^2 resolution (this could also be the latlong, for simplicity's sake)   - latitude and longitude data
- Re-implemented disso formatting because MS word formatting is a grumpy little bitch that doesn't like sharing its toys with other people. 

#### Friday 
- Implemented integrated CSV export (not all the way though) 
- More detail in image import pipeline 
  - Images are labeled with latitude and longitude in filename, which is extracted during label\_func
  - latitude and longitude are used to extract feature set - this can be used to create six separate networks that infer different gases, or one big network. We can test this, or just look it up 
- Testing is just going to be randomly selected from this image set so we can try it a number of different times and empirically see what works based on existing english data. There's likely to be quite a lot of data available just in the UK, so expanding it to the rest of the world is more of a matter of expanding the image import pipeline and sorting that revolving-chamber streaming idea so I don't use up an obscene amount of storage. An obscene amount of bandwidth, sure, but not storage. I wonder if I can programmatically create new colab sessions...
- 
