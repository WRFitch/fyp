{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fyp_preliminary_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WRFitch/fyp/blob/main/src/fyp_preliminary_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Ua09qtJqgw"
      },
      "source": [
        "## Setup\n",
        "\n",
        "\n",
        "*   Install & import necessary libraries\n",
        "*   Set up Earth Engine datastores. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSB1skY31RpR"
      },
      "source": [
        "!pip install fastai\n",
        "!pip install fastai2\n",
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1lreKK1bXw"
      },
      "source": [
        "import ee\n",
        "import folium\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from fastai import *\n",
        "from fastai.tabular import *\n",
        "from fastai.vision import *\n",
        "from fastai.vision.all import *\n",
        "from google.colab import drive\n",
        "from IPython.display import Image\n",
        "from pprint import pprint"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACtrAfnVnOjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30231817-df7d-4984-847b-ee3904d8e1f2"
      },
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#print(fastai.__version__)\n",
        "print(folium.__version__)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=0GF9i0ESMxz4oV6NPaZTi_6SrvuI_U4WS08w2Y1PM7Y&code_challenge_method=S256\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below. \n",
            "Enter verification code: 4/1AY0e-g4Ir5SmXauK8JsUF6K4CJfjLZ2R44_Rmi55tmsiE3Rp-G21R8a_cM4\n",
            "\n",
            "Successfully saved authorization token.\n",
            "Mounted at /content/drive\n",
            "0.8.3\n",
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZUKXEfUiEtZ"
      },
      "source": [
        "# Dataset import\n",
        "\n",
        "### Import the following datasets into Google Drive\n",
        "\n",
        "*   [Sentinel-2 Satellite photography](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR)\n",
        "*   [Sentinel-5 Precursor Data](https://developers.google.com/earth-engine/datasets/catalog/sentinel)\n",
        "  *   [Aerosol](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_AER_AI)\n",
        "  *   [Cloud](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CLOUD)\n",
        "  *   [Carbon Monoxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CO)\n",
        "  *   [Formaldehyde](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_HCHO)\n",
        "  *   [Nitrogen Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_NO2)\n",
        "  *   [Ozone](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_O3)\n",
        "  *   [Sulphur Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_SO2)\n",
        "  *   [Methane](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CH4)\n",
        "*   [ODIAC Fossil Fuel CO2 Emissions](https://db.cger.nies.go.jp/dataset/ODIAC/DL_odiac2019.html)\n",
        "\n",
        "### TODO\n",
        "- Systematise this into functions so I can easily select and make changes based on data resolution or climate dataset\n",
        "- Import CO2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQZlTc5Rf-LC"
      },
      "source": [
        "# Earth engine username, used to import classified image into ee assets folder\n",
        "USERNAME = 'wrfitch'\n",
        "OUTPUT_DIR = USERNAME + \"/out/\"\n",
        "\n",
        "# Define image collections for each dataset to be used \n",
        "s2 = ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "s5_CO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\n",
        "s5_HCHO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_HCHO\") \n",
        "s5_NO2 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_NO2\")\n",
        "s5_O3 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_O3\")\n",
        "s5_SO2 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_SO2\")\n",
        "s5_CH4 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CH4\")\n",
        "#TODO import CO2 dataset\n",
        "\n",
        "# Define dataset boundaries for britain and london \n",
        "# TODO work out polygon segmentation algo - there's probably a clever algorithm for this, but I could also just iterate \n",
        "#      through simple squares that fit my bandwidth and storage constraints. I run out of memory when using the gbr \n",
        "#      polygon anyway, so an iterative approach is necessary. \n",
        "great_britain = ee.Geometry.Polygon(\n",
        "        [[[-1.836112801004015, 59.808076330562756],\n",
        "          [-8.779472176004015, 58.82140293049428],\n",
        "          [-7.988456551004015, 55.71069203454839],\n",
        "          [-11.196464363504015, 54.42753859549109],\n",
        "          [-11.328300301004015, 50.967746003015044],\n",
        "          [-9.526542488504015, 50.77361752815123],\n",
        "          [-6.274589363504015, 51.81776248652293],\n",
        "          [-5.395683113504015, 51.21615275310099],\n",
        "          [-6.582206551004015, 49.56332371186494],\n",
        "          [-3.110526863504015, 49.904165426606255],\n",
        "          [1.240059073995985, 50.80139967619036],\n",
        "          [2.426582511495985, 52.33095407387208],\n",
        "          [1.767402823995985, 53.4183511305661],\n",
        "          [0.5369340739959849, 53.44453305344514],\n",
        "          [-1.616386238504015, 56.32474216074427],\n",
        "          [-0.7814253010040151, 57.805828290000164]]])\n",
        "\n",
        "london = ee.Geometry.Polygon(\n",
        "        [[[-1.0666833726431624, 51.89360084338857],\n",
        "          [-0.9321008531119124, 51.38908166135181],\n",
        "          [-0.18503054061191238, 51.08470683562287],\n",
        "          [0.4741491468881076, 51.193274483099074],\n",
        "          [0.9822668226693576, 51.60282356474035],\n",
        "          [0.2269567640756076, 52.071221592742454]]])\n",
        "\n",
        "# import variables\n",
        "# Could the start and end dates \n",
        "start_date = '2020-01-01'\n",
        "end_date = '2020-12-31'\n",
        "vis_palette = ['black', 'blue', 'purple', 'cyan', 'green', 'yellow', 'red']\n",
        "\n",
        "# export variables\n",
        "test_dir = \"test/\"\n",
        "train_dir = \"train/\"\n",
        "lr_dir = \"low_resolution/\"\n",
        "hr_dir = \"high_resolution/\"\n",
        "img_dimensions = \"224x224\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZcXFgkSf2Nr"
      },
      "source": [
        "# define utility methods - TODO remove to utils script \n",
        "\n",
        "# pre-filter to remove clouds - we can add them back in as data points from sentinel 5 if necessary\n",
        "def maskS2clouds(image) :\n",
        "  qa = image.select('QA60');\n",
        "\n",
        "  # Bits 10 and 11 are clouds and cirrus, respectively.\n",
        "  cloudBitMask = 1 << 10\n",
        "  cirrusBitMask = 1 << 11\n",
        "\n",
        "  # Both flags should be set to zero, indicating clear conditions.\n",
        "  mask = qa.bitwiseAnd(cloudBitMask).eq(0).And( \\\n",
        "         qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
        "\n",
        "  return image.updateMask(mask).divide(10000)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liIZ0y0JnSjY"
      },
      "source": [
        "# Import datasets \n",
        "# TODO analyse whether these min/max values are valid, recalibrate for highest variance where necessary. Separate values\n",
        "#      may be necessary for different samples - for example, the perfect calibration for the UK won't work on the world. \n",
        "# TODO analyse whether it makes sense to analyse these on a highly localised level\n",
        "\n",
        "# High-resolution satellite photograph \n",
        "s2_img = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
        "                  .filterDate(start_date, end_date) \\\n",
        "                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
        "                  .filterBounds(great_britain) \\\n",
        "                  .map(maskS2clouds).median()\n",
        "s2_id = s2_img.getMapId({'bands': ['B4', 'B3', 'B2'], \\\n",
        "                        'min': 0, \\\n",
        "                        'max': 0.3})\n",
        "\n",
        "# Carbon monoxide\n",
        "# Minmax scale is a bit off - recalibrate for Britain\n",
        "CO_img = s5_CO.filterDate(start_date, end_date) \\\n",
        "              .filterBounds(great_britain) \\\n",
        "              .select('CO_column_number_density').mean()\n",
        "CO_id = CO_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0, \\\n",
        "    'max': 0.05})\n",
        "\n",
        "# Formaldehyde\n",
        "# Minmax scale is a bit off - recalibrate for Britain\n",
        "HCHO_img = s5_HCHO.filterDate(start_date, end_date) \\\n",
        "                  .filterBounds(great_britain) \\\n",
        "                  .select('tropospheric_HCHO_column_number_density').mean()\n",
        "HCHO_id = HCHO_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0003})\n",
        "\n",
        "# Nitrogen Dioxide\n",
        "NO2_img = s5_NO2.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select('tropospheric_NO2_column_number_density').mean()\n",
        "NO2_id = NO2_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0002})\n",
        "\n",
        "# Ozone\n",
        "O3_img = s5_O3.filterDate(start_date, end_date) \\\n",
        "              .filterBounds(great_britain) \\\n",
        "              .select('O3_column_number_density').mean()\n",
        "O3_id = O3_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.12, \\\n",
        "    'max': 0.15})\n",
        "\n",
        "# Sulphur Dioxide\n",
        "SO2_img = s5_SO2.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select('SO2_column_number_density').mean()\n",
        "SO2_id = SO2_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0005})\n",
        "\n",
        "# Methane\n",
        "CH4_img = s5_CH4.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select('CH4_column_volume_mixing_ratio_dry_air').mean()\n",
        "CH4_id = CH4_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 1750, \\\n",
        "    'max': 1900})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCsfkjOE8y_H"
      },
      "source": [
        "# Visualise data on a Folium map \n",
        "# TODO find a valid attr value to replace the current val.\n",
        "map = folium.Map(location=[51.5, 0.1], \\\n",
        "                    prefer_canvas=True)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=s2_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=CO_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Carbon Monoxide',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=HCHO_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Formaldehyde',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=NO2_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Nitrogen Dioxide',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=O3_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Ozone',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=SO2_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Sulphur Dioxide',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=CH4_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Methane',\n",
        "  ).add_to(map)\n",
        "  \n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAowukAY38ho"
      },
      "source": [
        "# take a sample of the image at the points given and add a random column\n",
        "# TODO combine datasets into one. can tabular recommenders include images? \n",
        "# TOOD Save a small partition in google drive, then work on getting the next via a thread. \n",
        "#      This should also start the training process, then delete the small partition in the drive. \n",
        "\n",
        "# A range of samples to create an iterative downscaler. These could be more evenly spaced. \n",
        "# \"collection\" parameter may also need to be iteratively sampled during training cycles\n",
        "# TODO random column is a placeholder for data sorting - remove when possible! (I could sort by some other parameter, \n",
        "#      but that would influence the sampling)\n",
        "s2_1000m_smpl = s2_img.sampleRegions(\n",
        "    #collection = great_britain,\n",
        "    collection = london,\n",
        "    scale = 1000).randomColumn()\n",
        "s2_500m_smpl = s2_img.sampleRegions(\n",
        "    collection = london,\n",
        "    scale = 500).randomColumn()\n",
        "s2_100m_smpl = s2_img.sampleRegions(\n",
        "    collection = great_britain,\n",
        "    scale = 100).randomColumn()\n",
        "\n",
        "# Reaching an unlikely level of precision here. The convolution map from a 1km scale is very unlikely to have enough \n",
        "# detail to reconstruct individual roads and houses, but given this volume of data it's not unreasonable to assume some \n",
        "# improvement is still possible. \n",
        "s2_50m_smpl = s2_img.sampleRegions(\n",
        "    collection = great_britain,\n",
        "    scale = 50).randomColumn()\n",
        "s2_10m_smpl = s2_img.sampleRegions(\n",
        "    collection = great_britain,\n",
        "    scale = 10).randomColumn()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey3K53jf7ySK"
      },
      "source": [
        "# Make sure you can see the output bucket.  You must have write access.\n",
        "# not used because we're sticking to drive until it becomes a problem. \n",
        "#print('Found Cloud Storage bucket.' if tf.io.gfile.exists('gs://' + OUTPUT_BUCKET) \n",
        "#    else 'Can not find output Cloud Storage bucket.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmEHb54ge4tb"
      },
      "source": [
        "# Define train and test sets, randomly sampling both datasets. \n",
        "# TODO ensure data is synchronised by grid reference. How is resolution handled? how are images upscaled? are images stored by grid reference? \n",
        "# TODO calibrate test sample - a 30% test set might be overdoing it. \n",
        "train_input = s2_1000m_smpl.filter(ee.Filter.lt('random', 0.7))\n",
        "train_output = s2_500m_smpl.filter(ee.Filter.lt('random', 0.7))\n",
        "\n",
        "test_input = s2_1000m_smpl.filter(ee.Filter.gte('random', 0.7))\n",
        "test_output = s2_500m_smpl.filter(ee.Filter.gte('random', 0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVMq7rTG44S5"
      },
      "source": [
        "# Export images from imagecollections to drive\n",
        "# image dimensions can be defined at this stage, as can scaling, format,\n",
        "\n",
        "# Region can also be defined, allowing iteration here. Minimising the size of the loop is paramount, given the \n",
        "# functional paradigm EE uses.\n",
        "\n",
        "# Skipping empty tiles is necessary to ensure data integrity. However, it may be possible to infer data for empty values. \n",
        "\n",
        "\"\"\"\n",
        "export_hires_train_data = ee.batch.Export.table.toDrive(\n",
        "    collection = train_input,\n",
        "    description = \"exporting high-resolution training data\",\n",
        "    folder = train_dir + hr_dir,\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\n",
        "export_lores_train_data = ee.batch.Export.table.toDrive(\n",
        "    collection = train_output,\n",
        "    description = \"exporting low-resolution training data\",\n",
        "    folder = train_dir + lr_dir,\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\n",
        "export_hires_test_data = ee.batch.Export.table.toDrive(\n",
        "    collection = test_input,\n",
        "    description = \"exporting high-resolution testing data\",\n",
        "    folder = test_dir + hr_dir,\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\n",
        "export_lores_test_data = ee.batch.Export.table.toDrive(\n",
        "    collection = test_output,\n",
        "    description = \"exporting low-resolution testing data\",\n",
        "    folder = test_dir + lr_dir,\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "export_500m = ee.batch.Export.table.toDrive(\n",
        "    collection = s2_500m_smpl,\n",
        "    description = \"exporting data at 500m resolution\",\n",
        "    folder = \"500m\",\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\n",
        "export_1000m = ee.batch.Export.table.toDrive(\n",
        "    collection = s2_1000m_smpl,\n",
        "    description = \"exporting data at 1000m resolution\",\n",
        "    folder = \"1000m\",\n",
        "    fileFormat = 'TFRecord'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24-vv6e30OX1"
      },
      "source": [
        "#export_hires_train_data.start()\n",
        "#export_lores_train_data.start()\n",
        "#export_hires_test_data.start()\n",
        "#export_lores_test_data.start()\n",
        "\n",
        "export_1000m.start()\n",
        "export_500m.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8EmHO0toOxl"
      },
      "source": [
        "# get images & check they're ok.\n",
        "#path = untar_data(URLs.PETS)/'images'\n",
        "\n",
        "image_export_options = {\n",
        "    'patchDimensions': [256, 256],\n",
        "    'maxFileSize': 104857600,\n",
        "    'compressed':True \n",
        "}\n",
        "\n",
        "# Setup the task.\n",
        "export_1000m_img = ee.batch.Export.image.toDrive(\n",
        "  description = '1000m_scale_img',\n",
        "  fileFormat = 'TFRecord',\n",
        "  #fileDimensions = [224],\n",
        "  folder = \"1000m/imgs\",\n",
        "  formatOptions = image_export_options,\n",
        "  image = s2_img,\n",
        "  region = london,\n",
        "  scale = 1000,\n",
        ")\n",
        "\n",
        "export_500m_img = ee.batch.Export.image.toDrive(\n",
        "  description = '500m_scale_img',\n",
        "  fileFormat = 'TFRecord',\n",
        "  #fileDimensions = \"224\",\n",
        "  folder = \"500m/imgs\",\n",
        "  formatOptions = image_export_options,\n",
        "  image = s2_img,\n",
        "  region = london,\n",
        "  scale = 500,\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4n3shd6oQyN"
      },
      "source": [
        "# Start the task.\n",
        "export_1000m_img.start()\n",
        "export_500m_img.start()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccYR0r4p0NKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649544e9-a956-45fe-8357-aee6337d6394"
      },
      "source": [
        "pprint(ee.batch.Task.list())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<Task EXPORT_IMAGE: 500m_scale_img (READY)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (READY)>,\n",
            " <Task EXPORT_IMAGE: Image Export (FAILED)>,\n",
            " <Task EXPORT_IMAGE: Image Export (FAILED)>,\n",
            " <Task EXPORT_IMAGE: Image Export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution training data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 20200129T110209_20200129T110442_T31UCT (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 20200119T110259_20200119T110309_T30UXC (COMPLETED)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0qXxD7XoXTw"
      },
      "source": [
        "# Monitoring an individual task\n",
        "\n",
        "import time\n",
        "\n",
        "while image_task.active():\n",
        "  print('Polling for task (id: {}).'.format(image_task.id))\n",
        "  time.sleep(30)\n",
        "print('Done with image export.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWeGM3jenhqM"
      },
      "source": [
        "# Fastai & tensorflow processing\n",
        "\n",
        "### TODO\n",
        "- Untar data into a normal format for normal people, using tensorflow\n",
        "- Access datasets for fast.ai retraining. 2-400 images was sufficient for object recog, maybe double that for top-down sat photos?\n",
        "- Train an upscaling unet that can do 2x upscaling \n",
        "- __Save the satellite upscaler, including a local copy__\n",
        "- Transfer the upscaler to use methane data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW_zmBRh3LtR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "06fddf54-650b-41a9-a353-26d252459b84"
      },
      "source": [
        "# get images using tensorflow \n",
        "fastai.file_extract(\"1000m/Image Export-00001.tfrecord.gz\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8802ff636eff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get images using tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1000m/Image Export-00001.tfrecord.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'fastai' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T32KDTidwsw"
      },
      "source": [
        "# preprocess images \n",
        "# Slice low-res images \n",
        "# upscale low-res ones to be the same size as the required input \n",
        "# resize high-res test outputs to required output\n",
        "# utilise image transformations to expand dataset as much as possible\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmFTAyLi3_IY"
      },
      "source": [
        "type(get_image_files(path))\n",
        "print(get_image_files(path)[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}