{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fyp_preliminary_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "16Ua09qtJqgw"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WRFitch/fyp/blob/main/src/fyp_preliminary_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Ua09qtJqgw"
      },
      "source": [
        "## Setup\n",
        "\n",
        "\n",
        "*   Install & import necessary libraries\n",
        "*   Set up Earth Engine datastores. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a75WOxVUmS03",
        "outputId": "012d7d22-1270-44a4-ec83-90f3accc022c"
      },
      "source": [
        "!pip uninstall fastai"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling fastai-2.2.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/fastai-2.2.2.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/fastai/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled fastai-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSB1skY31RpR"
      },
      "source": [
        "!pip install -U --no-cache-dir fastai\n",
        "#!pip install fastai2\n",
        "#!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1lreKK1bXw"
      },
      "source": [
        "# TODO organise imports based on utility and location.\n",
        "import ee\n",
        "import folium\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from fastai import *\n",
        "from fastai.tabular import *\n",
        "from fastai.vision import *\n",
        "from fastai.vision.all import *\n",
        "from google.colab import drive\n",
        "from osgeo import gdal\n",
        "from PIL import Image\n",
        "from pprint import pprint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACtrAfnVnOjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e96062-da23-4cea-84fe-9fdf22fe90ae"
      },
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=a7E55dFufHxCO-IROpHGv-fB4YA9JbpYmAIN8QkO0gU&code_challenge_method=S256\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below. \n",
            "Enter verification code: 4/1AY0e-g5FXmDo6Mj4cGlEU1COX73CHdQdW5v1mT2c-qekmFi3vupiM-O6Dyw\n",
            "\n",
            "Successfully saved authorization token.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLUtYn4SndcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98112533-21ef-4f1e-bac1-8a8a8e8dc3a6"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "#print(fastai.__version__)\n",
        "print(folium.__version__)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "0.8.3\n",
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZUKXEfUiEtZ"
      },
      "source": [
        "# Dataset import\n",
        "\n",
        "### Import the following datasets into Google Drive\n",
        "\n",
        "*   [Sentinel-2 Satellite photography](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR)\n",
        "*   [Sentinel-5 Precursor Data](https://developers.google.com/earth-engine/datasets/catalog/sentinel)\n",
        "  *   [Aerosol](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_AER_AI)\n",
        "  *   [Cloud](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CLOUD)\n",
        "  *   [Carbon Monoxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CO)\n",
        "  *   [Formaldehyde](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_HCHO)\n",
        "  *   [Nitrogen Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_NO2)\n",
        "  *   [Ozone](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_O3)\n",
        "  *   [Sulphur Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_SO2)\n",
        "  *   [Methane](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CH4)\n",
        "*   [ODIAC Fossil Fuel CO2 Emissions](https://db.cger.nies.go.jp/dataset/ODIAC/DL_odiac2019.html)\n",
        "\n",
        "### TODO\n",
        "- Systematise this into functions so I can easily select and make changes based on data resolution or climate dataset\n",
        "- Import CO2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQZlTc5Rf-LC"
      },
      "source": [
        "# Earth engine username, used to import classified image into ee assets folder\n",
        "USERNAME = 'wrfitch'\n",
        "OUTPUT_DIR = USERNAME + \"/out/\"\n",
        "\n",
        "# Define image collections for each dataset to be used \n",
        "s2 = ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "s5_CO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\n",
        "s5_HCHO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_HCHO\")\n",
        "s5_NO2 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_NO2\")\n",
        "s5_O3 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_O3\")\n",
        "s5_SO2 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_SO2\")\n",
        "s5_CH4 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CH4\")\n",
        "#TODO import CO2 dataset\n",
        "\n",
        "# Define dataset boundaries for britain and london \n",
        "# TODO work out polygon segmentation algo - there's probably a clever algorithm for this, but I could also just iterate \n",
        "#      through simple squares that fit my bandwidth and storage constraints. I run out of memory when using the gbr \n",
        "#      polygon anyway, so an iterative approach is necessary. \n",
        "great_britain = ee.Geometry.Polygon(\n",
        "        [[[-1.836112801004015, 59.808076330562756],\n",
        "          [-8.779472176004015, 58.82140293049428],\n",
        "          [-7.988456551004015, 55.71069203454839],\n",
        "          [-11.196464363504015, 54.42753859549109],\n",
        "          [-11.328300301004015, 50.967746003015044],\n",
        "          [-9.526542488504015, 50.77361752815123],\n",
        "          [-6.274589363504015, 51.81776248652293],\n",
        "          [-5.395683113504015, 51.21615275310099],\n",
        "          [-6.582206551004015, 49.56332371186494],\n",
        "          [-3.110526863504015, 49.904165426606255],\n",
        "          [1.240059073995985, 50.80139967619036],\n",
        "          [2.426582511495985, 52.33095407387208],\n",
        "          [1.767402823995985, 53.4183511305661],\n",
        "          [0.5369340739959849, 53.44453305344514],\n",
        "          [-1.616386238504015, 56.32474216074427],\n",
        "          [-0.7814253010040151, 57.805828290000164]]])\n",
        "\n",
        "london = ee.Geometry.Polygon(\n",
        "        [[[-1.0666833726431624, 51.89360084338857],\n",
        "          [-0.9321008531119124, 51.38908166135181],\n",
        "          [-0.18503054061191238, 51.08470683562287],\n",
        "          [0.4741491468881076, 51.193274483099074],\n",
        "          [0.9822668226693576, 51.60282356474035],\n",
        "          [0.2269567640756076, 52.071221592742454]]])\n",
        "\n",
        "# import variables\n",
        "# Could the start and end dates \n",
        "start_date = '2020-01-01'\n",
        "end_date = '2020-12-31'\n",
        "vis_palette = ['black', 'blue', 'purple', 'cyan', 'green', 'yellow', 'red']\n",
        "\n",
        "# export variables\n",
        "img_dimensions = \"224x224\"\n",
        "drive_path = \"/content/drive/MyDrive/\"\n",
        "geotiff = 'GeoTIFF'\n",
        "tfrecord = 'TFRecord'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZcXFgkSf2Nr"
      },
      "source": [
        "# define utility methods - TODO remove to utils script \n",
        "\n",
        "# pre-filter to remove clouds - we can add them back in as data points from sentinel 5 if necessary\n",
        "def maskS2clouds(image) :\n",
        "  qa = image.select('QA60');\n",
        "\n",
        "  # Bits 10 and 11 are clouds and cirrus, respectively.\n",
        "  cloudBitMask = 1 << 10\n",
        "  cirrusBitMask = 1 << 11\n",
        "\n",
        "  # Both flags should be set to zero, indicating clear conditions.\n",
        "  mask = qa.bitwiseAnd(cloudBitMask).eq(0).And( \\\n",
        "         qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
        "\n",
        "  return image.updateMask(mask).divide(10000)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liIZ0y0JnSjY"
      },
      "source": [
        "# Import datasets \n",
        "# TODO analyse whether these min/max values are valid, recalibrate for highest variance where necessary. Separate values\n",
        "#      may be necessary for different samples - for example, the perfect calibration for the UK won't work on the world. \n",
        "# TODO analyse whether it makes sense to analyse these on a highly localised level\n",
        "\n",
        "# High-resolution satellite photograph \n",
        "s2_img = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
        "                  .filterDate(start_date, end_date) \\\n",
        "                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
        "                  .filterBounds(great_britain) \\\n",
        "                  .map(maskS2clouds).median()\n",
        "s2_id = s2_img.getMapId({'bands': ['B4', 'B3', 'B2'], \\\n",
        "                        'min': 0, \\\n",
        "                        'max': 0.3})\n",
        "\n",
        "# Carbon monoxide\n",
        "# Minmax scale is a bit off - recalibrate for Britain\n",
        "CO_img = s5_CO.filterDate(start_date, end_date) \\\n",
        "              .filterBounds(great_britain) \\\n",
        "              .select('CO_column_number_density').mean()\n",
        "CO_id = CO_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0, \\\n",
        "    'max': 0.05})\n",
        "\n",
        "# Formaldehyde\n",
        "# Minmax scale is a bit off - recalibrate for Britain\n",
        "HCHO_img = s5_HCHO.filterDate(start_date, end_date) \\\n",
        "                  .filterBounds(great_britain) \\\n",
        "                  .select('tropospheric_HCHO_column_number_density').mean()\n",
        "HCHO_id = HCHO_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0003})\n",
        "\n",
        "# Nitrogen Dioxide\n",
        "NO2_img = s5_NO2.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select('tropospheric_NO2_column_number_density').mean()\n",
        "NO2_id = NO2_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0002})\n",
        "\n",
        "# Ozone\n",
        "O3_img = s5_O3.filterDate(start_date, end_date) \\\n",
        "              .filterBounds(great_britain) \\\n",
        "              .select('O3_column_number_density').mean()\n",
        "O3_id = O3_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.12, \\\n",
        "    'max': 0.15})\n",
        "\n",
        "# Sulphur Dioxide\n",
        "SO2_img = s5_SO2.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select('SO2_column_number_density').mean()\n",
        "SO2_id = SO2_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0005})\n",
        "\n",
        "# Methane\n",
        "CH4_img = s5_CH4.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select('CH4_column_volume_mixing_ratio_dry_air').mean()\n",
        "CH4_id = CH4_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 1750, \\\n",
        "    'max': 1900})"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCsfkjOE8y_H"
      },
      "source": [
        "# Visualise data on a Folium map \n",
        "# TODO find a valid attr value to replace the current val.\n",
        "map = folium.Map(location=[51.5, 0.1], \\\n",
        "                    prefer_canvas=True)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=s2_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=CO_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Carbon Monoxide',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=HCHO_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Formaldehyde',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=NO2_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Nitrogen Dioxide',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=O3_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Ozone',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=SO2_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Sulphur Dioxide',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=CH4_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Methane',\n",
        "  ).add_to(map)\n",
        "  \n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAowukAY38ho"
      },
      "source": [
        "# take a sample of the image at the points given and add a random column\n",
        "# TODO combine datasets into one. can tabular recommenders include images? \n",
        "# TOOD Save a small partition in google drive, then work on getting the next via a thread. \n",
        "#      This should also start the training process, then delete the small partition in the drive. \n",
        "\n",
        "# A range of samples to create an iterative downscaler. These could be more evenly spaced. \n",
        "# \"collection\" parameter may also need to be iteratively sampled during training cycles\n",
        "# TODO random column is a placeholder for data sorting - remove when possible! (I could sort by some other parameter, \n",
        "#      but that would influence the sampling)\n",
        "s2_1000m_smpl = s2_img.sampleRegions(\n",
        "    collection = london,\n",
        "    scale = 1000).randomColumn()\n",
        "s2_500m_smpl = s2_img.sampleRegions(\n",
        "    collection = london,\n",
        "    scale = 500).randomColumn()\n",
        "s2_100m_smpl = s2_img.sampleRegions(\n",
        "    collection = london,\n",
        "    scale = 100).randomColumn()\n",
        "\n",
        "# Reaching an unlikely level of precision here. The convolution map from a 1km scale is very unlikely to have enough \n",
        "# detail to reconstruct individual roads and houses, but given this volume of data it's not unreasonable to assume some \n",
        "# improvement is still possible.\n",
        "s2_50m_smpl = s2_img.sampleRegions(\n",
        "    collection = london,\n",
        "    scale = 50).randomColumn()\n",
        "s2_10m_smpl = s2_img.sampleRegions(\n",
        "    collection = london,\n",
        "    scale = 10).randomColumn()"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey3K53jf7ySK"
      },
      "source": [
        "# Make sure you can see the output bucket.  You must have write access.\n",
        "# not used because we're sticking to drive until it becomes a problem. \n",
        "#print('Found Cloud Storage bucket.' if tf.io.gfile.exists('gs://' + OUTPUT_BUCKET) \n",
        "#    else 'Can not find output Cloud Storage bucket.')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmEHb54ge4tb"
      },
      "source": [
        "# Define train and test sets, randomly sampling both datasets. \n",
        "# TODO ensure data is synchronised by grid reference. How is resolution handled? how are images upscaled? are images stored by grid reference? \n",
        "# TODO calibrate test sample - a 30% test set might be overdoing it. \n",
        "# train_input = s2_1000m_smpl.filter(ee.Filter.lt('random', 0.7))\n",
        "# train_output = s2_500m_smpl.filter(ee.Filter.lt('random', 0.7))\n",
        "\n",
        "# test_input = s2_1000m_smpl.filter(ee.Filter.gte('random', 0.7))\n",
        "# test_output = s2_500m_smpl.filter(ee.Filter.gte('random', 0.7))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVMq7rTG44S5"
      },
      "source": [
        "# Export images from imagecollections to drive\n",
        "# image dimensions can be defined at this stage, as can scaling, format,\n",
        "\n",
        "# Region can also be defined, allowing iteration here. Minimising the size of the loop is paramount, given the \n",
        "# functional paradigm EE uses.\n",
        "\n",
        "# Skipping empty tiles is necessary to ensure data integrity. However, it may be possible to infer data for empty values. \n",
        "\n",
        "export_1000m = ee.batch.Export.table.toDrive(\n",
        "    collection = s2_1000m_smpl,\n",
        "    description = \"exporting data at 1000m resolution\",\n",
        "    folder = \"1000m\",\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\n",
        "export_500m = ee.batch.Export.table.toDrive(\n",
        "    collection = s2_500m_smpl,\n",
        "    description = \"exporting data at 500m resolution\",\n",
        "    folder = \"500m\",\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\n",
        "export_100m = ee.batch.Export.table.toDrive(\n",
        "    collection = s2_1000m_smpl,\n",
        "    description = \"exporting data at 100m resolution\",\n",
        "    folder = \"100m\",\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\n",
        "export_50m = ee.batch.Export.table.toDrive(\n",
        "    collection = s2_1000m_smpl,\n",
        "    description = \"exporting data at 50m resolution\",\n",
        "    folder = \"50m\",\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\n",
        "export_10m = ee.batch.Export.table.toDrive(\n",
        "    collection = s2_1000m_smpl,\n",
        "    description = \"exporting data at 10m resolution\",\n",
        "    folder = \"10m\",\n",
        "    fileFormat = 'TFRecord'\n",
        ")"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24-vv6e30OX1"
      },
      "source": [
        "#export_hires_train_data.start()\n",
        "#export_lores_train_data.start()\n",
        "#export_hires_test_data.start()\n",
        "#export_lores_test_data.start()\n",
        "\n",
        "export_1000m.start()\n",
        "export_500m.start()\n",
        "export_100m.start()\n",
        "export_50m.start()\n",
        "export_10m.start()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8EmHO0toOxl"
      },
      "source": [
        "# get images & check they're ok.\n",
        "#path = untar_data(URLs.PETS)/'images'\n",
        "\n",
        "image_export_options = {\n",
        "    'patchDimensions': [256, 256],\n",
        "    'maxFileSize': 104857600,\n",
        "    'compressed':True \n",
        "}\n",
        "\n",
        "# Setup the task.\n",
        "export_1000m_img = ee.batch.Export.image.toDrive(\n",
        "  description = '1000m_scale_img',\n",
        "  fileFormat = geotiff,\n",
        "  #fileDimensions = [224],\n",
        "  folder = \"1000m\",\n",
        "  #formatOptions = image_export_options,\n",
        "  image = s2_img,\n",
        "  region = london,\n",
        "  scale = 1000,\n",
        ")\n",
        "\n",
        "export_500m_img = ee.batch.Export.image.toDrive(\n",
        "  description = '500m_scale_img',\n",
        "  fileFormat = geotiff,\n",
        "  #fileDimensions = \"224\",\n",
        "  folder = \"500m\",\n",
        "  #formatOptions = image_export_options,\n",
        "  image = s2_img,\n",
        "  region = london,\n",
        "  scale = 500,\n",
        ")\n",
        "\n",
        "export_100m_img = ee.batch.Export.image.toDrive(\n",
        "  description = '100m_scale_img',\n",
        "  fileFormat = geotiff,\n",
        "  folder = \"100m\",\n",
        "  image = s2_img,\n",
        "  region = london,\n",
        "  scale = 100,\n",
        ")\n",
        "\n",
        "export_50m_img = ee.batch.Export.image.toDrive(\n",
        "  description = '50m_scale_img',\n",
        "  fileFormat = geotiff,\n",
        "  #fileDimensions = \"224\",\n",
        "  folder = \"50m\",\n",
        "  #formatOptions = image_export_options,\n",
        "  image = s2_img,\n",
        "  region = london,\n",
        "  scale = 50,\n",
        ")\n",
        "\n",
        "export_10m_img = ee.batch.Export.image.toDrive(\n",
        "  description = '10m_scale_img',\n",
        "  fileFormat = geotiff,\n",
        "  #fileDimensions = \"224\",\n",
        "  folder = \"10m\",\n",
        "  #formatOptions = image_export_options,\n",
        "  image = s2_img,\n",
        "  region = london,\n",
        "  scale = 10,\n",
        ")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4n3shd6oQyN"
      },
      "source": [
        "# Start the task.\n",
        "#export_1000m_img.start()\n",
        "#export_500m_img.start()\n",
        "export_100m_img.start()\n",
        "export_50m_img.start()\n",
        "export_10m_img.start()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccYR0r4p0NKI"
      },
      "source": [
        "pprint(ee.batch.Task.list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0qXxD7XoXTw"
      },
      "source": [
        "# Monitoring an individual task\n",
        "\n",
        "# import time\n",
        "\n",
        "# while image_task.active():\n",
        "#   print('Polling for task (id: {}).'.format(image_task.id))\n",
        "#   time.sleep(30)\n",
        "# print('Done with image export.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWeGM3jenhqM"
      },
      "source": [
        "# Fastai & tensorflow processing\n",
        "\n",
        "### TODO\n",
        "- Convert GeoTIFF to PNG\n",
        "- Access datasets for fast.ai retraining. 2-400 images was sufficient for object recog, maybe double that for top-down sat photos?\n",
        "- Train an upscaling unet that can do 2x upscaling \n",
        "- __Save the satellite upscaler, including a local copy__\n",
        "- Transfer the upscaler to use methane data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojueZsqRrH5P"
      },
      "source": [
        "# converts geotiff to png, using selected bands. There seems to be a limited range of functional bands, including only 3 \n",
        "# being available for a non-transparent image. The bands also display in black-and-white when displayed individually. \n",
        "\n",
        "def geotiffToPng(tif_path):\n",
        "  #TODO remap to ARGB to get more defined brightness data\n",
        "  options_list = [\n",
        "    '-ot Byte',\n",
        "    '-of PNG',\n",
        "    '-b 4',\n",
        "    '-b 3',\n",
        "    '-b 2',\n",
        "    '-scale'\n",
        "  ]\n",
        "  options_string = \" \".join(options_list)\n",
        "\n",
        "  yourpath = os.path.join(drive_path, tif_path)\n",
        "  print(yourpath)\n",
        "  \n",
        "  for root, dirs, files in os.walk(yourpath, topdown=False):\n",
        "    for name in files:\n",
        "      print(os.path.join(root, name))\n",
        "      if os.path.splitext(os.path.join(root, name))[1].lower() in {\".tiff\", \".tif\"}:\n",
        "        path = os.path.splitext(os.path.join(root, name))[0]\n",
        "        if os.path.isfile(path + \".png\"):\n",
        "          print(\"A png file already exists for %s\" % name)\n",
        "          #return\n",
        "        \n",
        "        gdal.Translate(\n",
        "          path + 'band_432.png',\n",
        "          path + '.tif',\n",
        "          options=options_string\n",
        "        )\n",
        "\n",
        "# get images \n",
        "geotiffToPng(\"100m\")\n",
        "#file_extract(drive_path + \"1000m/imgs/1000m_scale_img.tif\") #file_extract doesn't seem to want to play"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T32KDTidwsw"
      },
      "source": [
        "# preprocess images \n",
        "# Slice low-res images \n",
        "# upscale low-res ones to be the same size as the required input \n",
        "# resize high-res test outputs to required output\n",
        "# Map low-res input images to high-res output images using GeoTIFF metadata \n",
        "# utilise image transformations to expand dataset as much as possible - rotations & transforms should do the trick. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmFTAyLi3_IY"
      },
      "source": [
        "type(get_image_files(path))\n",
        "print(get_image_files(path)[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}