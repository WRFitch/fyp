{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fyp_preliminary_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WRFitch/fyp/blob/main/src/fyp_preliminary_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Ua09qtJqgw"
      },
      "source": [
        "## Setup\n",
        "\n",
        "\n",
        "*   Install & import necessary libraries\n",
        "*   Set up Earth Engine datastores. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a75WOxVUmS03",
        "outputId": "0daacf51-8ef5-4146-f251-36aec2febc39"
      },
      "source": [
        "!pip uninstall fastai"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling fastai-2.2.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/fastai-2.2.2.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/fastai/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled fastai-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSB1skY31RpR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "4a6479e9-b7df-47be-e436-f9bd87e93740"
      },
      "source": [
        "!pip install -U --no-cache-dir fastai\n",
        "#!pip install fastai2\n",
        "#!pip install tensorflow"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a5/b89118a8ea730d3e654f2794f65eb5249515525d34a5bca43517f3c91ba1/fastai-2.2.2-py3-none-any.whl (191kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 15.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 20.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30kB 14.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 40kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 51kB 18.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 61kB 17.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 71kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 81kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 92kB 11.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 102kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 112kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 122kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 133kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 143kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 153kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 163kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 174kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 184kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: fastcore<1.4,>=1.3.8 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.3.18)\n",
            "Requirement already satisfied, skipping upgrade: spacy in /usr/local/lib/python3.6/dist-packages (from fastai) (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pip in /usr/local/lib/python3.6/dist-packages (from fastai) (19.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: torchvision<0.9,>=0.8 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: fastprogress>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fastai) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (20.8)\n",
            "Requirement already satisfied, skipping upgrade: torch<1.8,>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pillow>6.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (51.1.1)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fastai) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch<1.8,>=1.7.0->fastai) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<1.8,>=1.7.0->fastai) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch<1.8,>=1.7.0->fastai) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->fastai) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->fastai) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai) (3.4.0)\n",
            "Installing collected packages: fastai\n",
            "Successfully installed fastai-2.2.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "fastai"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1lreKK1bXw"
      },
      "source": [
        "import ee\n",
        "import folium\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from fastai import *\n",
        "from fastai.tabular import *\n",
        "from fastai.vision import *\n",
        "from fastai.vision.all import *\n",
        "from google.colab import drive\n",
        "from IPython.display import Image\n",
        "from pprint import pprint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACtrAfnVnOjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c6f2cb4-00e2-42a2-b0e4-26e7222a6554"
      },
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=2BmyGGAoWGg2W8h21TREclFRgq_NWaj9cw1KYtgh_UM&code_challenge_method=S256\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below. \n",
            "Enter verification code: 4/1AY0e-g5ZgULStQRdQlYuvml1TskFd0cvYE6Pp-MPCubrauclexidB_7W89w\n",
            "\n",
            "Successfully saved authorization token.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "0.8.3\n",
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLUtYn4SndcT"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "#print(fastai.__version__)\n",
        "print(folium.__version__)\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZUKXEfUiEtZ"
      },
      "source": [
        "# Dataset import\n",
        "\n",
        "### Import the following datasets into Google Drive\n",
        "\n",
        "*   [Sentinel-2 Satellite photography](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR)\n",
        "*   [Sentinel-5 Precursor Data](https://developers.google.com/earth-engine/datasets/catalog/sentinel)\n",
        "  *   [Aerosol](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_AER_AI)\n",
        "  *   [Cloud](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CLOUD)\n",
        "  *   [Carbon Monoxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CO)\n",
        "  *   [Formaldehyde](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_HCHO)\n",
        "  *   [Nitrogen Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_NO2)\n",
        "  *   [Ozone](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_O3)\n",
        "  *   [Sulphur Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_SO2)\n",
        "  *   [Methane](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CH4)\n",
        "*   [ODIAC Fossil Fuel CO2 Emissions](https://db.cger.nies.go.jp/dataset/ODIAC/DL_odiac2019.html)\n",
        "\n",
        "### TODO\n",
        "- Systematise this into functions so I can easily select and make changes based on data resolution or climate dataset\n",
        "- Import CO2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQZlTc5Rf-LC"
      },
      "source": [
        "# Earth engine username, used to import classified image into ee assets folder\n",
        "USERNAME = 'wrfitch'\n",
        "OUTPUT_DIR = USERNAME + \"/out/\"\n",
        "\n",
        "# Define image collections for each dataset to be used \n",
        "s2 = ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "s5_CO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\n",
        "s5_HCHO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_HCHO\") \n",
        "s5_NO2 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_NO2\")\n",
        "s5_O3 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_O3\")\n",
        "s5_SO2 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_SO2\")\n",
        "s5_CH4 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CH4\")\n",
        "#TODO import CO2 dataset\n",
        "\n",
        "# Define dataset boundaries for britain and london \n",
        "# TODO work out polygon segmentation algo - there's probably a clever algorithm for this, but I could also just iterate \n",
        "#      through simple squares that fit my bandwidth and storage constraints. I run out of memory when using the gbr \n",
        "#      polygon anyway, so an iterative approach is necessary. \n",
        "great_britain = ee.Geometry.Polygon(\n",
        "        [[[-1.836112801004015, 59.808076330562756],\n",
        "          [-8.779472176004015, 58.82140293049428],\n",
        "          [-7.988456551004015, 55.71069203454839],\n",
        "          [-11.196464363504015, 54.42753859549109],\n",
        "          [-11.328300301004015, 50.967746003015044],\n",
        "          [-9.526542488504015, 50.77361752815123],\n",
        "          [-6.274589363504015, 51.81776248652293],\n",
        "          [-5.395683113504015, 51.21615275310099],\n",
        "          [-6.582206551004015, 49.56332371186494],\n",
        "          [-3.110526863504015, 49.904165426606255],\n",
        "          [1.240059073995985, 50.80139967619036],\n",
        "          [2.426582511495985, 52.33095407387208],\n",
        "          [1.767402823995985, 53.4183511305661],\n",
        "          [0.5369340739959849, 53.44453305344514],\n",
        "          [-1.616386238504015, 56.32474216074427],\n",
        "          [-0.7814253010040151, 57.805828290000164]]])\n",
        "\n",
        "london = ee.Geometry.Polygon(\n",
        "        [[[-1.0666833726431624, 51.89360084338857],\n",
        "          [-0.9321008531119124, 51.38908166135181],\n",
        "          [-0.18503054061191238, 51.08470683562287],\n",
        "          [0.4741491468881076, 51.193274483099074],\n",
        "          [0.9822668226693576, 51.60282356474035],\n",
        "          [0.2269567640756076, 52.071221592742454]]])\n",
        "\n",
        "# import variables\n",
        "# Could the start and end dates \n",
        "start_date = '2020-01-01'\n",
        "end_date = '2020-12-31'\n",
        "vis_palette = ['black', 'blue', 'purple', 'cyan', 'green', 'yellow', 'red']\n",
        "\n",
        "# export variables\n",
        "test_dir = \"test/\"\n",
        "train_dir = \"train/\"\n",
        "lr_dir = \"low_resolution/\"\n",
        "hr_dir = \"high_resolution/\"\n",
        "img_dimensions = \"224x224\"\n",
        "drive_path = \"drive/MyDrive/\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZcXFgkSf2Nr"
      },
      "source": [
        "# define utility methods - TODO remove to utils script \n",
        "\n",
        "# pre-filter to remove clouds - we can add them back in as data points from sentinel 5 if necessary\n",
        "def maskS2clouds(image) :\n",
        "  qa = image.select('QA60');\n",
        "\n",
        "  # Bits 10 and 11 are clouds and cirrus, respectively.\n",
        "  cloudBitMask = 1 << 10\n",
        "  cirrusBitMask = 1 << 11\n",
        "\n",
        "  # Both flags should be set to zero, indicating clear conditions.\n",
        "  mask = qa.bitwiseAnd(cloudBitMask).eq(0).And( \\\n",
        "         qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
        "\n",
        "  return image.updateMask(mask).divide(10000)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liIZ0y0JnSjY"
      },
      "source": [
        "# Import datasets \n",
        "# TODO analyse whether these min/max values are valid, recalibrate for highest variance where necessary. Separate values\n",
        "#      may be necessary for different samples - for example, the perfect calibration for the UK won't work on the world. \n",
        "# TODO analyse whether it makes sense to analyse these on a highly localised level\n",
        "\n",
        "# High-resolution satellite photograph \n",
        "s2_img = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
        "                  .filterDate(start_date, end_date) \\\n",
        "                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
        "                  .filterBounds(great_britain) \\\n",
        "                  .map(maskS2clouds).median()\n",
        "s2_id = s2_img.getMapId({'bands': ['B4', 'B3', 'B2'], \\\n",
        "                        'min': 0, \\\n",
        "                        'max': 0.3})\n",
        "\n",
        "# Carbon monoxide\n",
        "# Minmax scale is a bit off - recalibrate for Britain\n",
        "CO_img = s5_CO.filterDate(start_date, end_date) \\\n",
        "              .filterBounds(great_britain) \\\n",
        "              .select('CO_column_number_density').mean()\n",
        "CO_id = CO_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0, \\\n",
        "    'max': 0.05})\n",
        "\n",
        "# Formaldehyde\n",
        "# Minmax scale is a bit off - recalibrate for Britain\n",
        "HCHO_img = s5_HCHO.filterDate(start_date, end_date) \\\n",
        "                  .filterBounds(great_britain) \\\n",
        "                  .select('tropospheric_HCHO_column_number_density').mean()\n",
        "HCHO_id = HCHO_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0003})\n",
        "\n",
        "# Nitrogen Dioxide\n",
        "NO2_img = s5_NO2.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select('tropospheric_NO2_column_number_density').mean()\n",
        "NO2_id = NO2_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0002})\n",
        "\n",
        "# Ozone\n",
        "O3_img = s5_O3.filterDate(start_date, end_date) \\\n",
        "              .filterBounds(great_britain) \\\n",
        "              .select('O3_column_number_density').mean()\n",
        "O3_id = O3_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.12, \\\n",
        "    'max': 0.15})\n",
        "\n",
        "# Sulphur Dioxide\n",
        "SO2_img = s5_SO2.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select('SO2_column_number_density').mean()\n",
        "SO2_id = SO2_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0005})\n",
        "\n",
        "# Methane\n",
        "CH4_img = s5_CH4.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select('CH4_column_volume_mixing_ratio_dry_air').mean()\n",
        "CH4_id = CH4_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 1750, \\\n",
        "    'max': 1900})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCsfkjOE8y_H"
      },
      "source": [
        "# Visualise data on a Folium map \n",
        "# TODO find a valid attr value to replace the current val.\n",
        "map = folium.Map(location=[51.5, 0.1], \\\n",
        "                    prefer_canvas=True)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=s2_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=CO_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Carbon Monoxide',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=HCHO_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Formaldehyde',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=NO2_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Nitrogen Dioxide',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=O3_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Ozone',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=SO2_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Sulphur Dioxide',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=CH4_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Methane',\n",
        "  ).add_to(map)\n",
        "  \n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAowukAY38ho"
      },
      "source": [
        "# take a sample of the image at the points given and add a random column\n",
        "# TODO combine datasets into one. can tabular recommenders include images? \n",
        "# TOOD Save a small partition in google drive, then work on getting the next via a thread. \n",
        "#      This should also start the training process, then delete the small partition in the drive. \n",
        "\n",
        "# A range of samples to create an iterative downscaler. These could be more evenly spaced. \n",
        "# \"collection\" parameter may also need to be iteratively sampled during training cycles\n",
        "# TODO random column is a placeholder for data sorting - remove when possible! (I could sort by some other parameter, \n",
        "#      but that would influence the sampling)\n",
        "s2_1000m_smpl = s2_img.sampleRegions(\n",
        "    #collection = great_britain,\n",
        "    collection = london,\n",
        "    scale = 1000).randomColumn()\n",
        "s2_500m_smpl = s2_img.sampleRegions(\n",
        "    collection = london,\n",
        "    scale = 500).randomColumn()\n",
        "s2_100m_smpl = s2_img.sampleRegions(\n",
        "    collection = great_britain,\n",
        "    scale = 100).randomColumn()\n",
        "\n",
        "# Reaching an unlikely level of precision here. The convolution map from a 1km scale is very unlikely to have enough \n",
        "# detail to reconstruct individual roads and houses, but given this volume of data it's not unreasonable to assume some \n",
        "# improvement is still possible. \n",
        "s2_50m_smpl = s2_img.sampleRegions(\n",
        "    collection = great_britain,\n",
        "    scale = 50).randomColumn()\n",
        "s2_10m_smpl = s2_img.sampleRegions(\n",
        "    collection = great_britain,\n",
        "    scale = 10).randomColumn()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey3K53jf7ySK"
      },
      "source": [
        "# Make sure you can see the output bucket.  You must have write access.\n",
        "# not used because we're sticking to drive until it becomes a problem. \n",
        "#print('Found Cloud Storage bucket.' if tf.io.gfile.exists('gs://' + OUTPUT_BUCKET) \n",
        "#    else 'Can not find output Cloud Storage bucket.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmEHb54ge4tb"
      },
      "source": [
        "# Define train and test sets, randomly sampling both datasets. \n",
        "# TODO ensure data is synchronised by grid reference. How is resolution handled? how are images upscaled? are images stored by grid reference? \n",
        "# TODO calibrate test sample - a 30% test set might be overdoing it. \n",
        "train_input = s2_1000m_smpl.filter(ee.Filter.lt('random', 0.7))\n",
        "train_output = s2_500m_smpl.filter(ee.Filter.lt('random', 0.7))\n",
        "\n",
        "test_input = s2_1000m_smpl.filter(ee.Filter.gte('random', 0.7))\n",
        "test_output = s2_500m_smpl.filter(ee.Filter.gte('random', 0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVMq7rTG44S5"
      },
      "source": [
        "# Export images from imagecollections to drive\n",
        "# image dimensions can be defined at this stage, as can scaling, format,\n",
        "\n",
        "# Region can also be defined, allowing iteration here. Minimising the size of the loop is paramount, given the \n",
        "# functional paradigm EE uses.\n",
        "\n",
        "# Skipping empty tiles is necessary to ensure data integrity. However, it may be possible to infer data for empty values. \n",
        "\n",
        "\"\"\"\n",
        "export_hires_train_data = ee.batch.Export.table.toDrive(\n",
        "    collection = train_input,\n",
        "    description = \"exporting high-resolution training data\",\n",
        "    folder = train_dir + hr_dir,\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\n",
        "export_lores_train_data = ee.batch.Export.table.toDrive(\n",
        "    collection = train_output,\n",
        "    description = \"exporting low-resolution training data\",\n",
        "    folder = train_dir + lr_dir,\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\n",
        "export_hires_test_data = ee.batch.Export.table.toDrive(\n",
        "    collection = test_input,\n",
        "    description = \"exporting high-resolution testing data\",\n",
        "    folder = test_dir + hr_dir,\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\n",
        "export_lores_test_data = ee.batch.Export.table.toDrive(\n",
        "    collection = test_output,\n",
        "    description = \"exporting low-resolution testing data\",\n",
        "    folder = test_dir + lr_dir,\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "export_500m = ee.batch.Export.table.toDrive(\n",
        "    collection = s2_500m_smpl,\n",
        "    description = \"exporting data at 500m resolution\",\n",
        "    folder = \"500m\",\n",
        "    fileFormat = 'TFRecord'\n",
        ")\n",
        "\n",
        "export_1000m = ee.batch.Export.table.toDrive(\n",
        "    collection = s2_1000m_smpl,\n",
        "    description = \"exporting data at 1000m resolution\",\n",
        "    folder = \"1000m\",\n",
        "    fileFormat = 'TFRecord'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24-vv6e30OX1"
      },
      "source": [
        "#export_hires_train_data.start()\n",
        "#export_lores_train_data.start()\n",
        "#export_hires_test_data.start()\n",
        "#export_lores_test_data.start()\n",
        "\n",
        "export_1000m.start()\n",
        "export_500m.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8EmHO0toOxl"
      },
      "source": [
        "# get images & check they're ok.\n",
        "#path = untar_data(URLs.PETS)/'images'\n",
        "\n",
        "image_export_options = {\n",
        "    'patchDimensions': [256, 256],\n",
        "    'maxFileSize': 104857600,\n",
        "    'compressed':True \n",
        "}\n",
        "\n",
        "# Setup the task.\n",
        "export_1000m_img = ee.batch.Export.image.toDrive(\n",
        "  description = '1000m_scale_img',\n",
        "  fileFormat = 'TFRecord',\n",
        "  #fileDimensions = [224],\n",
        "  folder = \"1000m/imgs\",\n",
        "  formatOptions = image_export_options,\n",
        "  image = s2_img,\n",
        "  region = london,\n",
        "  scale = 1000,\n",
        ")\n",
        "\n",
        "export_500m_img = ee.batch.Export.image.toDrive(\n",
        "  description = '500m_scale_img',\n",
        "  fileFormat = 'TFRecord',\n",
        "  #fileDimensions = \"224\",\n",
        "  folder = \"500m/imgs\",\n",
        "  formatOptions = image_export_options,\n",
        "  image = s2_img,\n",
        "  region = london,\n",
        "  scale = 500,\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4n3shd6oQyN"
      },
      "source": [
        "# Start the task.\n",
        "export_1000m_img.start()\n",
        "export_500m_img.start()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccYR0r4p0NKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649544e9-a956-45fe-8357-aee6337d6394"
      },
      "source": [
        "pprint(ee.batch.Task.list())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<Task EXPORT_IMAGE: 500m_scale_img (READY)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (READY)>,\n",
            " <Task EXPORT_IMAGE: Image Export (FAILED)>,\n",
            " <Task EXPORT_IMAGE: Image Export (FAILED)>,\n",
            " <Task EXPORT_IMAGE: Image Export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution training data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 20200129T110209_20200129T110442_T31UCT (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 20200119T110259_20200119T110309_T30UXC (COMPLETED)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0qXxD7XoXTw"
      },
      "source": [
        "# Monitoring an individual task\n",
        "\n",
        "import time\n",
        "\n",
        "while image_task.active():\n",
        "  print('Polling for task (id: {}).'.format(image_task.id))\n",
        "  time.sleep(30)\n",
        "print('Done with image export.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWeGM3jenhqM"
      },
      "source": [
        "# Fastai & tensorflow processing\n",
        "\n",
        "### TODO\n",
        "- Untar data into a normal format for normal people, using tensorflow\n",
        "- Access datasets for fast.ai retraining. 2-400 images was sufficient for object recog, maybe double that for top-down sat photos?\n",
        "- Train an upscaling unet that can do 2x upscaling \n",
        "- __Save the satellite upscaler, including a local copy__\n",
        "- Transfer the upscaler to use methane data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW_zmBRh3LtR"
      },
      "source": [
        "# get images using tensorflow \n",
        "#file_extract(drive_path + \"1000m/Image Export-00001.tfrecord.gz\") #file_extract doesn't seem to want to play\n",
        "import_dataset = tf.data.TFRecordDataset(drive_path + \"1000m/Image Export-00001.tfrecord.gz\", compression_type='GZIP')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T32KDTidwsw"
      },
      "source": [
        "# preprocess images \n",
        "# Slice low-res images \n",
        "# upscale low-res ones to be the same size as the required input \n",
        "# resize high-res test outputs to required output\n",
        "# utilise image transformations to expand dataset as much as possible\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmFTAyLi3_IY"
      },
      "source": [
        "type(get_image_files(path))\n",
        "print(get_image_files(path)[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}