{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fyp_data_import_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DlFd1y1OIXxk",
        "jkq4mhmfIwO_",
        "sTZipoKOJNyg",
        "MglKunWI_fTv",
        "f9l197PkYqGG"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WRFitch/fyp/blob/main/src/fyp_data_import_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBYK4VpU-2tn"
      },
      "source": [
        "# Data Import Pipeline\n",
        "\n",
        "This code is still under construction, and is therefore very very bad in places. \n",
        "\n",
        "### TODO\n",
        "- Import CO2 dataset\n",
        "- Figure out a way of iterating through existing images and displaying the area currently covered by my dataset on a map. \n",
        "- Define and import other regions of interest - stick to cities and suburbs for now, since that will have the best health data. Converting this to include rural or rocky areas is an increase in feature set. \n",
        "- Figure out how accurate the image exports are\n",
        "  - Are the points definitely centered on the given coordinates? \n",
        "  - is there a way of standardising lighting? \n",
        "- add file indexing into one CSV with all our existing latlong exports, so we're not constantly querying the filesystem. \n",
        "- list exported files into a CSV \n",
        "- Move data processing methods into their own pipeline "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Ua09qtJqgw"
      },
      "source": [
        "## Setup\n",
        "*   Import necessary libraries\n",
        "*   Import fyputil module\n",
        "*   Set up Earth Engine authentication and mount google drive  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1lreKK1bXw"
      },
      "source": [
        "import ee\n",
        "import os\n",
        "import pandas as pd \n",
        "\n",
        "from google.colab import drive\n",
        "from osgeo import gdal\n",
        "from pprint import pprint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACtrAfnVnOjt"
      },
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P59lmKIpX8_5"
      },
      "source": [
        "%rm -rf /content/fyp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD9oir7Sh9pX"
      },
      "source": [
        "# Import FYP repo so we can access fyputil common library \n",
        "%cd /content\n",
        "!git clone https://github.com/WRFitch/fyp.git\n",
        "\n",
        "# Import fyputil library\n",
        "%cd fyp/src/fyputil\n",
        "import constants as c\n",
        "import ee_constants as eec\n",
        "import ee_utils as eeutil\n",
        "import fyp_utils as fyputil\n",
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZUKXEfUiEtZ"
      },
      "source": [
        "# Dataset import\n",
        "\n",
        "### Import the following datasets into Google Drive\n",
        "\n",
        "*   [Sentinel-2 Satellite photography](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR)\n",
        "*   [Sentinel-5 Precursor Data](https://developers.google.com/earth-engine/datasets/catalog/sentinel)\n",
        "  *   [Carbon Monoxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CO)\n",
        "  *   [Formaldehyde](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_HCHO)\n",
        "  *   [Nitrogen Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_NO2)\n",
        "  *   [Ozone](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_O3)\n",
        "  *   [Sulphur Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_SO2)\n",
        "  *   [Methane](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CH4)\n",
        "*   [ODIAC Fossil Fuel CO2 Emissions](https://db.cger.nies.go.jp/dataset/ODIAC/DL_odiac2019.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTZipoKOJNyg"
      },
      "source": [
        "### Visualise Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X8Qxxmkg7rN"
      },
      "source": [
        "eec.map "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHIkD4AIJRD9"
      },
      "source": [
        "### Export Data\n",
        "\n",
        "Exports as .csv tables and GeoTIFF images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9l197PkYqGG"
      },
      "source": [
        "#### Exporting CSVs\n",
        "\n",
        "This method of getting the data is very very stupid, but also it does exactly what I need. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyfnXNdtC-Vg"
      },
      "source": [
        "# Only once this is completed can you move forward and get pictures from these spreadsheets.\n",
        "for ghg_img in eec.ghg_imgs:\n",
        "  csv_name = ghg_img.getInfo().get('bands')[0].get('id')\n",
        "  print(csv_name)\n",
        "  #eeutil.exportTableFromImage(ghg_img, eec.south_east, 1000, c.export_dir, csv_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqzw-XDH5DDO"
      },
      "source": [
        "#### Getting Images From CSV Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuxNYYoumLDO"
      },
      "source": [
        "eeutil.getImgsFromCsv(f\"{c.data_dir}/{c.SO2_band}.csv\", eec.s2_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDu_zz3Vs-Xf"
      },
      "source": [
        "#pprint(ee.batch.Task.list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWeGM3jenhqM"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhaMrmlzNE6-"
      },
      "source": [
        "fyputil.geotiffToPng(\"big_geotiff\", f\"{c.export_dir}/png_224\", rm_artifacts=False)\n",
        "fyputil.moveFilesByExtension(\"big_geotiff\", f\"{c.export_dir}/geotiff_224\", \".tif\")\n",
        "fyputil.moveFilesByExtension(\"big_geotiff\", f\"{c.export_dir}/png_224\", \".png\")\n",
        "fyputil.rmConversionArtifacts(\"big_geotiff\", rmTif=False, rmXml=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vkByB6_ZhwH"
      },
      "source": [
        "fyputil.geotiffToPng(f\"{c.export_dir}/geotiff_224\", f\"{c.export_dir}/png_224\", rm_artifacts=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWkxiQtyZxT9"
      },
      "source": [
        "fyputil.moveFilesByExtension(f\"{c.export_dir}/geotiff_224\", f\"{c.export_dir}/png_224\", \".png\")\n",
        "fyputil.rmConversionArtifacts(f\"{c.export_dir}/geotiff_224\", rmTif=False, rmXml=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkr6RzUt_atC"
      },
      "source": [
        "# Cleaning up if things go a bit wrong\n",
        "fyputil.moveFilesByExtension(f\"{c.export_dir}/geotiff\", f\"{export_dir}/png\", \".png\")\n",
        "fyputil.rmConversionArtifacts(f\"{c.export_dir}/geotiff\", rmTif=False, rmXml=True)\n",
        "fyputil.moveFilesByExtension(c.geotiff_dir, f\"{c.export_dir}/geotiff\", \".tif\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU0Z3mwYD5_p"
      },
      "source": [
        "###Incorporate all CSVs into one\n",
        "\n",
        "This could be more efficient, but it's only processed once at the minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGUu7dyWn9jh"
      },
      "source": [
        "fyputil.parseCsvCoords(f\"{c.data_dir}/se-{c.CO_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/se-{c.HCHO_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/se-{c.NO2_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/se-{c.O3_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/se-{c.SO2_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/se-{c.CH4_band}.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyNxAy0AsQu2"
      },
      "source": [
        "# Parse CSVs into pandas dataframes\n",
        "# TODO rewrite so we aren't deleting columns directly - do it properly! Incorporate these into one csv export in the \n",
        "#      output pipeline \n",
        "co_df = pd.read_csv(f\"{c.data_dir}/se-{c.CO_band}.csv\")\n",
        "del co_df[\".geo\"]\n",
        "hcho_df = pd.read_csv(f\"{c.data_dir}/se-{c.HCHO_band}.csv\")\n",
        "del hcho_df[\".geo\"]\n",
        "no2_df = pd.read_csv(f\"{c.data_dir}/se-{c.NO2_band}.csv\")\n",
        "del no2_df[\".geo\"]\n",
        "o3_df = pd.read_csv(f\"{c.data_dir}/se-{c.O3_band}.csv\")\n",
        "del o3_df[\".geo\"]\n",
        "so2_df = pd.read_csv(f\"{c.data_dir}/se-{c.SO2_band}.csv\")\n",
        "del so2_df[\".geo\"]\n",
        "ch4_df = pd.read_csv(f\"{c.data_dir}/se-{c.CH4_band}.csv\")\n",
        "del ch4_df[\".geo\"]\n",
        "\n",
        "# Incorporate individual csvs into one ghg dataframe. Badly. \n",
        "# TODO fix this so we aren't repeating the same thing over and over\n",
        "mrg_params = ['longitude', 'latitude']\n",
        "# somehow this means \"intersect\". We're taking the intersect so we know we have common values. \n",
        "mrg_type = 'inner'\n",
        "\n",
        "intersect = pd.merge(so2_df, ch4_df, how=mrg_type, on=mrg_params)\n",
        "intersect = pd.merge(intersect, co_df, how=mrg_type, on=mrg_params)\n",
        "intersect = pd.merge(intersect, hcho_df, how=mrg_type, on=mrg_params)\n",
        "intersect = pd.merge(intersect, no2_df, how=mrg_type, on=mrg_params)\n",
        "intersect = pd.merge(intersect, o3_df, how=mrg_type, on=mrg_params)\n",
        "\n",
        "print(intersect.shape)\n",
        "intersect.iloc[0:4] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_fPSymy0bJV"
      },
      "source": [
        "# Hacky again, but it'll do for now\n",
        "del intersect[\"system:index_x\"]\n",
        "del intersect[\"system:index_y\"]\n",
        "intersect.iloc[0:4] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbo-sIH-yDo9"
      },
      "source": [
        "raw_ghg_df = intersect.copy()\n",
        "\n",
        "for index, row in intersect.iterrows():\n",
        "  coords = (row.longitude, row.latitude)\n",
        "  #print(coords)\n",
        "  filepath = f\"{c.data_dir}/png_224/{coords[0]}_{coords[1]}.png\"\n",
        "  if not os.path.isfile(filepath):\n",
        "    print(f\"dropping {filepath} from row {index}\")\n",
        "    # TODO implement this in a way that doesn't suck. \n",
        "    raw_ghg_df = raw_ghg_df.drop(index=index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCdxrBST4xSr"
      },
      "source": [
        "print(intersect.shape)\n",
        "print(raw_ghg_df.shape)\n",
        "raw_ghg_df.iloc[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8VCVIjjyF18"
      },
      "source": [
        "intersect.to_csv(f\"{c.data_dir}/se-ghgs.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}