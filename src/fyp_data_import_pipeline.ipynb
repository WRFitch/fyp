{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fyp_data_import_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DlFd1y1OIXxk",
        "jkq4mhmfIwO_",
        "sTZipoKOJNyg",
        "MglKunWI_fTv",
        "f9l197PkYqGG"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WRFitch/fyp/blob/main/src/fyp_data_import_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBYK4VpU-2tn"
      },
      "source": [
        "# Data Import Pipeline\n",
        "\n",
        "This code is still under construction, and is therefore very very bad in places. \n",
        "\n",
        "### TODO\n",
        "- Import CO2 dataset\n",
        "- Extract unnecessary methods into normal python files and import where necessary. \n",
        "- Remove unnecessary variable changes where necessary - this stacks up all the JSON, making everything harder than it needs to be. \n",
        "  - Actually, it might not\n",
        "- Change unnecessary image imports to feature imports \n",
        "- Figure out a way of iterating through existing images and displaying the area currently covered by my dataset on a map. \n",
        "- Define and import other regions of interest - stick to cities and suburbs for now, since that will have the best health data. Converting this to include rural or rocky areas is an increase in feature set. \n",
        "- Figure out how accurate the image exports are\n",
        "  - Are the points definitely centered on the given coordinates? \n",
        "  - is there a way of standardising lighting? \n",
        "- add file indexing into one CSV with all our existing latlong exports, so we're not constantly querying the filesystem. \n",
        "- Begin exporting commonalities across notebooks, and incorporate a single pipeline that can be employed in a demonstration script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Ua09qtJqgw"
      },
      "source": [
        "## Setup\n",
        "*   Import necessary libraries\n",
        "*   Set up Earth Engine authentication and mount google drive  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1lreKK1bXw"
      },
      "source": [
        "import ee\n",
        "import folium\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "from osgeo import gdal\n",
        "from PIL import Image\n",
        "from pprint import pprint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACtrAfnVnOjt"
      },
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmRRGZyvamsq"
      },
      "source": [
        "%cd /content\n",
        "%rm -rf fyp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD9oir7Sh9pX"
      },
      "source": [
        "# Import FYP repo so we can access fyputil common library \n",
        "%cd /content\n",
        "!git clone https://github.com/WRFitch/fyp.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuepO1knXIZd",
        "outputId": "2853c7e3-f4f2-422d-8eb6-d89c8197e920"
      },
      "source": [
        "# Import fyputil library\n",
        "%cd fyp/src/fyputil\n",
        "import constants as c\n",
        "import ee_constants as eec\n",
        "import ee_utils as eeutil\n",
        "import fyp_utils as fyputil\n",
        "%cd /content"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'fyp'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (225/225), done.\u001b[K\n",
            "remote: Compressing objects: 100% (183/183), done.\u001b[K\n",
            "remote: Total 737 (delta 140), reused 86 (delta 42), pack-reused 512\u001b[K\n",
            "Receiving objects: 100% (737/737), 143.42 MiB | 19.47 MiB/s, done.\n",
            "Resolving deltas: 100% (395/395), done.\n",
            "/content/fyp/src/fyputil\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZUKXEfUiEtZ"
      },
      "source": [
        "# Dataset import\n",
        "\n",
        "### Import the following datasets into Google Drive\n",
        "\n",
        "*   [Sentinel-2 Satellite photography](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR)\n",
        "*   [Sentinel-5 Precursor Data](https://developers.google.com/earth-engine/datasets/catalog/sentinel)\n",
        "  *   [Carbon Monoxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CO)\n",
        "  *   [Formaldehyde](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_HCHO)\n",
        "  *   [Nitrogen Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_NO2)\n",
        "  *   [Ozone](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_O3)\n",
        "  *   [Sulphur Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_SO2)\n",
        "  *   [Methane](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CH4)\n",
        "*   [ODIAC Fossil Fuel CO2 Emissions](https://db.cger.nies.go.jp/dataset/ODIAC/DL_odiac2019.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlFd1y1OIXxk"
      },
      "source": [
        "##### Define Earth Engine Boundaries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuosygcHIfhO"
      },
      "source": [
        "# Define dataset boundaries for britain and london \n",
        "great_britain = eec.great_britain\n",
        "london = eec.london\n",
        "uxbridge = eec.uxbridge\n",
        "millennium_dome = eec.millennium_dome\n",
        "greenwich = eec.greenwich\n",
        "w_hemisphere = eec.west_hemisphere\n",
        "e_hemisphere = eec.east_hemisphere"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkq4mhmfIwO_"
      },
      "source": [
        "##### Define other implementation variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQZlTc5Rf-LC"
      },
      "source": [
        "# TODO move this into a CONSTANTS.py file\n",
        "# Earth engine username, used to import classified image into ee assets folder\n",
        "USERNAME = 'wrfitch'\n",
        "OUTPUT_DIR = USERNAME + \"/out/\"\n",
        "\n",
        "# Define collections for each dataset to be used \n",
        "s2 = eec.s2\n",
        "s5_CO = eec.s5_CO\n",
        "s5_HCHO = eec.s5_HCHO\n",
        "s5_NO2 = eec.s5_NO2\n",
        "s5_O3 = eec.s5_O3\n",
        "s5_SO2 = eec.s5_SO2\n",
        "s5_CH4 = eec.s5_CH4\n",
        "#TODO import CO2 dataset\n",
        "\n",
        "CO_band = c.CO_band\n",
        "HCHO_band = c.HCHO_band\n",
        "NO2_band = c.NO2_band\n",
        "O3_band = c.O3_band\n",
        "SO2_band = c.SO2_band\n",
        "CH4_band = c.CH4_band\n",
        "\n",
        "start_date = eec.start_date\n",
        "end_date = eec.end_date\n",
        "vis_palette = eec.vis_palette\n",
        "\n",
        "drive_path = c.drive_path\n",
        "export_dir = c.export_dir\n",
        "geotiff_dir = c.geotiff_dir"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liIZ0y0JnSjY"
      },
      "source": [
        "# Import datasets \n",
        "# TODO analyse whether these min/max values are valid, recalibrate for highest variance where necessary. Separate values\n",
        "#      may be necessary for different samples - for example, the perfect calibration for the UK won't work on the world. \n",
        "# TODO analyse whether it makes sense to analyse these on a highly localised level\n",
        "# Filterbounds doesn't even seem to work locally, so may as well remove it\n",
        "\n",
        "# pre-filter to remove clouds - we can add them back in as data points from sentinel 5 if necessary\n",
        "def maskS2clouds(image) :\n",
        "  qa = image.select('QA60');\n",
        "\n",
        "  # Bits 10 and 11 are clouds and cirrus, respectively.\n",
        "  cloud_bitmask = 1 << 10\n",
        "  cirrus_bitmask = 1 << 11\n",
        "\n",
        "  # Both flags should be set to zero, indicating clear conditions.\n",
        "  mask = qa.bitwiseAnd(cloud_bitmask).eq(0).And( \\\n",
        "         qa.bitwiseAnd(cirrus_bitmask).eq(0))\n",
        "\n",
        "  return image.updateMask(mask).divide(10000)\n",
        "\n",
        "# High-resolution satellite photograph \n",
        "s2_img = s2.filterDate(start_date, end_date) \\\n",
        "           .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
        "           .filterBounds(great_britain) \\\n",
        "           .map(maskS2clouds).median()\n",
        "s2_id = s2_img.getMapId({'bands': ['B4', 'B3', 'B2'], \\\n",
        "                        'min': 0, \\\n",
        "                        'max': 0.3})\n",
        "\n",
        "# Carbon monoxide\n",
        "# Minmax scale is a bit off - recalibrate for Britain \n",
        "CO_img = s5_CO.filterDate(start_date, end_date) \\\n",
        "              .filterBounds(great_britain) \\\n",
        "              .select(CO_band).mean()\n",
        "CO_id = CO_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0, \\\n",
        "    'max': 0.05})\n",
        "\n",
        "# Formaldehyde\n",
        "# Minmax scale is a bit off - recalibrate for Britain\n",
        "HCHO_img = s5_HCHO.filterDate(start_date, end_date) \\\n",
        "                  .filterBounds(great_britain) \\\n",
        "                  .select(HCHO_band).mean()\n",
        "HCHO_id = HCHO_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0003})\n",
        "\n",
        "# Nitrogen Dioxide\n",
        "NO2_img = s5_NO2.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select(NO2_band).mean()\n",
        "NO2_id = NO2_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0002})\n",
        "\n",
        "# Ozone\n",
        "O3_img = s5_O3.filterDate(start_date, end_date) \\\n",
        "              .filterBounds(great_britain) \\\n",
        "              .select(O3_band).mean()\n",
        "O3_id = O3_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.12, \\\n",
        "    'max': 0.15})\n",
        "\n",
        "# Sulphur Dioxide\n",
        "SO2_img = s5_SO2.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select(SO2_band).mean()\n",
        "SO2_id = SO2_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0005})\n",
        "\n",
        "# Methane\n",
        "CH4_img = s5_CH4.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select(CH4_band).mean()\n",
        "CH4_id = CH4_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 1750, \\\n",
        "    'max': 1900})"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbfrhtfaeYGD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSRiplMExL_j"
      },
      "source": [
        "# For easier iteration down the line. I know I'm not supposed to, but google can't tell me what to do, even if it's a good idea!\n",
        "ghg_imgs = [CO_img, HCHO_img, NO2_img, O3_img, SO2_img, CH4_img]\n",
        "ghg_ids = [CO_id, HCHO_id, NO2_id, O3_id, SO2_id, CH4_id]\n",
        "ghg_fcs = [CO_fc, HCHO_fc, NO2_fc, O3_fc, SO2_fc, CH4_fc]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTZipoKOJNyg"
      },
      "source": [
        "### Visualise Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCsfkjOE8y_H"
      },
      "source": [
        "# Visualise data on a Folium map \n",
        "# Attribution has to stay earthengine.google.com, since that's where these maps came from. \n",
        "map = folium.Map(\n",
        "    location = [51.5, 0.1], \n",
        "    prefer_canvas = True)\n",
        "\n",
        "layerOpacity = 0.5\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = s2_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'satellite photography median composite '\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = CO_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'Carbon Monoxide',\n",
        "    opacity = layerOpacity\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = HCHO_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'Formaldehyde',\n",
        "    opacity = layerOpacity\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = NO2_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'Nitrogen Dioxide',\n",
        "    opacity = layerOpacity\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = O3_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'Ozone',\n",
        "    opacity = layerOpacity\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = SO2_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'Sulphur Dioxide',\n",
        "    opacity = layerOpacity\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = CH4_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'Methane',\n",
        "    opacity = layerOpacity\n",
        "  ).add_to(map)\n",
        "  \n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X8Qxxmkg7rN"
      },
      "source": [
        "eec.map "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHIkD4AIJRD9"
      },
      "source": [
        "### Export Data\n",
        "\n",
        "Exports as .csv tables and GeoTIFF images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MglKunWI_fTv"
      },
      "source": [
        "#### Define export methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56P_LOcgkimq"
      },
      "source": [
        "# All export methods export to the google drive defined above \n",
        "# TODO add checking for existing files in these methods.\n",
        "\n",
        "def exportTable(table, scale, folder=\"no_export_folder\", desc=\"no_desc\"):\n",
        "  ee.batch.Export.table.toDrive(\n",
        "    collection = table,\n",
        "    description = desc,\n",
        "    folder = folder,\n",
        "    fileFormat = \"CSV\"\n",
        "  ).start()\n",
        "\n",
        "# Export one table of the given image, at the scale and dimensions specified.\n",
        "def exportTableFromImage(image, polygon, scale, folder=\"no_export_folder\", desc=\"no_desc\"):\n",
        "  exportTable(sample(image, polygon, scale), scale, folder, desc)\n",
        "  \n",
        "# samples image into feature_collection. \n",
        "# TODO why is this called sample() ?\n",
        "def sample(img, region, scale):\n",
        "  return img.sampleRegions(\n",
        "      collection = region,\n",
        "      geometries = True,\n",
        "      scale = scale\n",
        "  )\n",
        "\n",
        "# Export one GeoTIFF image of the given image, at the scale and dimension specified. \n",
        "# TODO reevaluate image export options - description needs coordinates\n",
        "# maxPixels is just so it lets me export london at 10m/px. Dividing the dataset into 1km squares is the next step. \n",
        "def exportGeotiff(image, polygon, scale, folder=\"no_export_folder\", desc=\"no_desc\"):\n",
        "  ee.batch.Export.image.toDrive(\n",
        "    crs = 'EPSG:3857',\n",
        "    description = desc,\n",
        "    fileFormat = 'GeoTIFF',\n",
        "    folder = folder,\n",
        "    # formatOptions = image_export_options,\n",
        "    image = image,\n",
        "    maxPixels = 10e9,\n",
        "    region = polygon,\n",
        "    scale = scale\n",
        "  ).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imcye1aSmzok"
      },
      "source": [
        "#pprint(ee.batch.Task.list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9l197PkYqGG"
      },
      "source": [
        "#### Exporting CSVs\n",
        "\n",
        "This method of getting the data is very very stupid, but also it does exactly what I need. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyfnXNdtC-Vg"
      },
      "source": [
        "# Only once this is completed can you move forward and get pictures from these spreadsheets.\n",
        "for ghg_img in ghg_imgs:\n",
        "  csv_name = ghg_img.getInfo().get('bands')[0].get('id')\n",
        "  #exportTableFromImage(ghg_img,london, 1000, export_dir, csv_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqzw-XDH5DDO"
      },
      "source": [
        "#### Getting Images From CSV Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6m01ies5U-G"
      },
      "source": [
        "import csv \n",
        "import json\n",
        "import time\n",
        "\n",
        "# Gets a square kilometer with the given point object as the centroid. \n",
        "def getSqKmFromPoint(point):\n",
        "  return point.buffer(500).bounds()\n",
        "\n",
        "# Read in CSV from drive\n",
        "# This method is bad, and I should feel bad \n",
        "def getImgsFromCsv(csv_path):\n",
        "  with open(csv_path) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    firstRow = True\n",
        "\n",
        "    # The number of simultaneous exports can't go over 3 or 4 without taking a performance hit. \n",
        "    concurrent_exports = 3\n",
        "    export_buffer = []\n",
        "\n",
        "    for row in csv_reader:\n",
        "      if firstRow:\n",
        "        firstRow = False\n",
        "        continue\n",
        "\n",
        "      # coords are stored as [LONGITUDE, LATITUDE]\n",
        "      coords = json.loads(row[2]).get(\"coordinates\")\n",
        "      polygon = getSqKmFromPoint(ee.Geometry.Point(coords))\n",
        "      name = f\"{coords[0]}_{coords[1]}\"\n",
        "      tifname = name + \".tif\"\n",
        "\n",
        "      # skip files that have already been exported. \n",
        "      if os.path.isfile(f\"{drive_path}{export_dir}/pngs/{name}.png\") or \\\n",
        "          os.path.isfile(f\"{drive_path}{export_dir}/geotiff/{tifname}\") or \\\n",
        "          os.path.isfile(f\"{drive_path}{geotiff_dir}/{tifname}\"):\n",
        "        # TODO update to include variable log levels\n",
        "        #print(f\"skipping  {name} - file already exists!\")\n",
        "        continue\n",
        "        \n",
        "      export_buffer.append(tifname)\n",
        "      print(f\"exporting {tifname}\")\n",
        "      exportGeotiff(s2_img, polygon, 10, geotiff_dir, name)\n",
        "\n",
        "      # Block until export buffer is smaller than the maximum allowed concurrent exports. \n",
        "      while len(export_buffer) >= concurrent_exports:\n",
        "        time.sleep(5)\n",
        "        # when the files stored in the export buffer are found in the filesystem, remove them.\n",
        "        for filename in export_buffer:\n",
        "          if os.path.isfile(f\"{drive_path}{geotiff_dir}/{filename}\"): \n",
        "            export_buffer.remove(filename)\n",
        "            print(f\"exported  {filename}\")\n",
        "  \n",
        "  print(f\"exported {csv_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOfFnBtr9E3G"
      },
      "source": [
        "getImgsFromCsv(drive_path + export_dir + \"/SO2_column_number_density.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWeGM3jenhqM"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojueZsqRrH5P"
      },
      "source": [
        "# removes old geoTIFF images or xml conversion artifacts from the given directory. \n",
        "def rmArtifact(artifact_path, rmTif = False, rmXml = False):\n",
        "  if not os.path.isfile(artifact_path): return\n",
        "  if not (rmTif or rmXml): return\n",
        "  \n",
        "  extension = os.path.splitext(artifact_path)[1].lower()\n",
        "  if (extension == \".tif\" and rmTif) or \\\n",
        "      (extension == \".xml\" and rmXml): \n",
        "    print(f\"removing {artifact_path}\")\n",
        "    os.remove(artifact_path)\n",
        "\n",
        "def rmConversionArtifacts(path, rmTif = False, rmXml = False):\n",
        "  # No point checking all these files if we're not going to do anything \n",
        "  if not (rmTif or rmXml): return\n",
        "\n",
        "  parent_path = os.path.join(drive_path, path)\n",
        "  print(parent_path)\n",
        "\n",
        "  for root, dirs, files in os.walk(parent_path, topdown=True):\n",
        "    for name in files:\n",
        "      fullpath = os.path.join(root, name)\n",
        "      rmArtifact(fullpath, rmTif, rmXml)\n",
        "\n",
        "def geotiffToPng(tif_path, rm_artifacts = False):\n",
        "  # Define rgb bands and file extension\n",
        "  options_list = [\n",
        "    '-ot Byte',\n",
        "    '-of PNG',\n",
        "    '-b 4',\n",
        "    '-b 3',\n",
        "    '-b 2',\n",
        "    '-scale'\n",
        "  ]\n",
        "  options_string = \" \".join(options_list)\n",
        "  parent_path = os.path.join(drive_path, tif_path)\n",
        "\n",
        "  # Recursively walk through all files (this has to be simpler)\n",
        "  for root, dirs, files in os.walk(parent_path, topdown=False):\n",
        "    for name in files:\n",
        "      full_path = os.path.join(root, name)\n",
        "      print(full_path)\n",
        "      split_path = os.path.splitext(full_path)\n",
        "\n",
        "      if split_path[1].lower() != \".tif\": continue\n",
        "\n",
        "      path = split_path[0]\n",
        "      filename = path.split(\"/\")[-1]\n",
        "      if os.path.isfile(path + \".png\") or os.path.isfile(f\"{drive_path}{export_dir}/png/{filename}.png\"):\n",
        "        print(f\"A png file already exists for {full_path}\")\n",
        "        continue\n",
        "      \n",
        "      gdal.Translate(\n",
        "        path + '.png',\n",
        "        path + '.tif',\n",
        "        options = options_string\n",
        "      )\n",
        "      print(f\"Converted {filename} from GeoTIFF to PNG\")\n",
        "      if rm_artifacts: rmArtifact(full_path, True, True)\n",
        "\n",
        "# Move files from src to dest if they have the correct extension\n",
        "def moveFilesByExtension(src, dest, extension):\n",
        "  parent_path = os.path.join(drive_path, src)\n",
        "  print(parent_path)\n",
        "\n",
        "  for root, dirs, files in os.walk(parent_path, topdown=True):\n",
        "    for name in files:\n",
        "      full_path = os.path.join(root, name)\n",
        "      split_path = os.path.splitext(full_path)\n",
        "      if split_path[1].lower() == extension:\n",
        "        dest_path = full_path.replace(src, dest)\n",
        "        os.rename(full_path, dest_path)\n",
        "\n",
        "# TODO currently unused, since the rest of the code checks for duplicates before initiating an export. \n",
        "def deDupe(path):\n",
        "  print(\"removing duplicates\")\n",
        "  parent_path = os.path.join(drive_path, path)\n",
        "  print(parent_path)\n",
        "\n",
        "  for root, dirs, files in os.walk(parent_path, topdown=True):\n",
        "    for name in files:\n",
        "      # TODO update to check if the filename fits the intended format for coordinate images. \n",
        "      try:\n",
        "        float(name.split(\"_\")[-1][:-4])\n",
        "      except Exception:\n",
        "        print(f\"removing {name}\")\n",
        "        #os.remove(os.path.join(root, name))\n",
        "\n",
        "# TODO add file indexing into one CSV with all our latlong exports."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhaMrmlzNE6-"
      },
      "source": [
        "# add de-duplication back in once it actually does something. I've managed to avoid dupes so far anyway, so it's \n",
        "# unnecessary until it becomes an actual problem. YAGNI! \n",
        "# TODO parameterise filepaths in constants.py\n",
        "#deDupe(f\"{drive_path}{geotiff_dir}\")\n",
        "geotiffToPng(geotiff_dir, rm_artifacts=False)\n",
        "moveFilesByExtension(geotiff_dir, f\"{export_dir}/geotiff\", \".tif\")\n",
        "moveFilesByExtension(geotiff_dir, f\"{export_dir}/png\", \".png\")\n",
        "rmConversionArtifacts(geotiff_dir, rmTif=False, rmXml=True)\n",
        "\n",
        "#deDupe(f\"{drive_path}{export_dir}/png\")\n",
        "#deDupe(f\"{drive_path}{export_dir}/geotiff\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkr6RzUt_atC"
      },
      "source": [
        "# Cleaning up if things go a bit wrong\n",
        "moveFilesByExtension(f\"{export_dir}/geotiff\", f\"{export_dir}/png\", \".png\")\n",
        "rmConversionArtifacts(f\"{export_dir}/geotiff\", rmTif=False, rmXml=True)\n",
        "moveFilesByExtension(geotiff_dir, f\"{export_dir}/geotiff\", \".tif\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKNGcpcJkhrv"
      },
      "source": [
        "# Extracts lat and long volumes from .geo param in csv. \n",
        "def parseCsvCoords(csv_path):\n",
        "  print(csv_path)\n",
        "  test_suffix = \"_parsing_coordinates.csv\"\n",
        "  \n",
        "  with open(csv_path, 'r') as read_obj, \\\n",
        "      open(csv_path + test_suffix, 'w', newline='') as write_obj:\n",
        "    csv_reader = csv.reader(read_obj, delimiter=\",\")\n",
        "    csv_writer = csv.writer(write_obj, delimiter=\",\")\n",
        "\n",
        "    firstRow = True\n",
        "    for row in csv_reader:\n",
        "      if firstRow: \n",
        "        # break early if this method has already been applied to the given csv. \n",
        "        if \"longitude\" in row: \n",
        "          print(f\"{csv_path} has already been processed - exiting...\")\n",
        "          return \n",
        "        row.append(\"longitude\")\n",
        "        row.append(\"latitude\")\n",
        "        firstRow = False\n",
        "      else:\n",
        "        # Could string parsing be made more efficient? \n",
        "        # This method only needs to run once per csv so optimisation isn't that important. \n",
        "        coords = json.loads(row[2]).get(\"coordinates\")\n",
        "        row.append(coords[0])\n",
        "        row.append(coords[1])\n",
        "\n",
        "      csv_writer.writerow(row)\n",
        "  \n",
        "  os.remove(csv_path)\n",
        "  os.rename(csv_path + test_suffix, csv_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGUu7dyWn9jh"
      },
      "source": [
        "# TODO add these all into one csv file as part of import pipeline. \n",
        "parseCsvCoords(f\"{drive_path}{export_dir}/{CO_band}.csv\")\n",
        "parseCsvCoords(f\"{drive_path}{export_dir}/{HCHO_band}.csv\")\n",
        "parseCsvCoords(f\"{drive_path}{export_dir}/{NO2_band}.csv\")\n",
        "parseCsvCoords(f\"{drive_path}{export_dir}/{O3_band}.csv\")\n",
        "parseCsvCoords(f\"{drive_path}{export_dir}/{SO2_band}.csv\")\n",
        "parseCsvCoords(f\"{drive_path}{export_dir}/{CH4_band}.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}