{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fyp_data_import_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WRFitch/fyp/blob/main/src/fyp_data_import_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Ua09qtJqgw"
      },
      "source": [
        "## Setup\n",
        "\n",
        "\n",
        "*   Import necessary libraries\n",
        "*   Set up Earth Engine authentication and mount google drive  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1lreKK1bXw"
      },
      "source": [
        "import ee\n",
        "import folium\n",
        "\n",
        "from google.colab import drive\n",
        "from osgeo import gdal\n",
        "from PIL import Image\n",
        "from pprint import pprint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACtrAfnVnOjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac8c0ca-a3f9-448c-b4ac-52c178677cc3"
      },
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=mf6YGyTfBwRKjDKeGkVhOVK7eyZ9kaIfX-V3A6yhBe4&code_challenge_method=S256\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below. \n",
            "Enter verification code: 4/1AY0e-g6dxW4LRMpzFhp4GlGmItmM9k_DBfiJNanzJzcraNLYuxeVD-cfR4E\n",
            "\n",
            "Successfully saved authorization token.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLUtYn4SndcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ee8236-bf38-4927-b221-972f86481e10"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "print(folium.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "0.8.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZUKXEfUiEtZ"
      },
      "source": [
        "# Dataset import\n",
        "\n",
        "### Import the following datasets into Google Drive\n",
        "\n",
        "*   [Sentinel-2 Satellite photography](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR)\n",
        "*   [Sentinel-5 Precursor Data](https://developers.google.com/earth-engine/datasets/catalog/sentinel)\n",
        "  *   [Aerosol](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_AER_AI)\n",
        "  *   [Cloud](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CLOUD)\n",
        "  *   [Carbon Monoxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CO)\n",
        "  *   [Formaldehyde](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_HCHO)\n",
        "  *   [Nitrogen Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_NO2)\n",
        "  *   [Ozone](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_O3)\n",
        "  *   [Sulphur Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_SO2)\n",
        "  *   [Methane](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CH4)\n",
        "*   [ODIAC Fossil Fuel CO2 Emissions](https://db.cger.nies.go.jp/dataset/ODIAC/DL_odiac2019.html)\n",
        "\n",
        "### TODO\n",
        "- ~~Systematise this into functions so I can easily select and make changes based on data resolution or climate dataset~~\n",
        "- Import CO2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQZlTc5Rf-LC"
      },
      "source": [
        "# Earth engine username, used to import classified image into ee assets folder\n",
        "USERNAME = 'wrfitch'\n",
        "OUTPUT_DIR = USERNAME + \"/out/\"\n",
        "\n",
        "# Define image collections for each dataset to be used \n",
        "s2 = ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "s5_CO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\n",
        "s5_HCHO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_HCHO\")\n",
        "s5_NO2 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_NO2\")\n",
        "s5_O3 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_O3\")\n",
        "s5_SO2 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_SO2\")\n",
        "s5_CH4 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CH4\")\n",
        "#TODO import CO2 dataset\n",
        "\n",
        "# Define dataset boundaries for britain and london \n",
        "# TODO work out polygon segmentation algo - there's probably a clever algorithm for this, but I could also just iterate \n",
        "#      through simple squares that fit my bandwidth and storage constraints. I run out of memory when using the gbr \n",
        "#      polygon anyway, so an iterative approach is necessary. \n",
        "great_britain = ee.Geometry.Polygon(\n",
        "        [[[-1.836112801004015, 59.808076330562756],\n",
        "          [-8.779472176004015, 58.82140293049428],\n",
        "          [-7.988456551004015, 55.71069203454839],\n",
        "          [-11.196464363504015, 54.42753859549109],\n",
        "          [-11.328300301004015, 50.967746003015044],\n",
        "          [-9.526542488504015, 50.77361752815123],\n",
        "          [-6.274589363504015, 51.81776248652293],\n",
        "          [-5.395683113504015, 51.21615275310099],\n",
        "          [-6.582206551004015, 49.56332371186494],\n",
        "          [-3.110526863504015, 49.904165426606255],\n",
        "          [1.240059073995985, 50.80139967619036],\n",
        "          [2.426582511495985, 52.33095407387208],\n",
        "          [1.767402823995985, 53.4183511305661],\n",
        "          [0.5369340739959849, 53.44453305344514],\n",
        "          [-1.616386238504015, 56.32474216074427],\n",
        "          [-0.7814253010040151, 57.805828290000164]]])\n",
        "\n",
        "london = ee.Geometry.Polygon(\n",
        "        [[[-1.0666833726431624, 51.89360084338857],\n",
        "          [-0.9321008531119124, 51.38908166135181],\n",
        "          [-0.18503054061191238, 51.08470683562287],\n",
        "          [0.4741491468881076, 51.193274483099074],\n",
        "          [0.9822668226693576, 51.60282356474035],\n",
        "          [0.2269567640756076, 52.071221592742454]]])\n",
        "\n",
        "# Could the start and end dates be shifted or focused on one area, so emissions can be monitored across the seasons? \n",
        "# would that even be useful? \n",
        "start_date = '2020-01-01'\n",
        "end_date = '2020-12-31'\n",
        "vis_palette = ['black', 'blue', 'purple', 'cyan', 'green', 'yellow', 'red']\n",
        "\n",
        "drive_path = \"/content/drive/MyDrive/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTZipoKOJNyg"
      },
      "source": [
        "### Visualise Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liIZ0y0JnSjY"
      },
      "source": [
        "# Import datasets \n",
        "# TODO analyse whether these min/max values are valid, recalibrate for highest variance where necessary. Separate values\n",
        "#      may be necessary for different samples - for example, the perfect calibration for the UK won't work on the world. \n",
        "# TODO analyse whether it makes sense to analyse these on a highly localised level\n",
        "\n",
        "# pre-filter to remove clouds - we can add them back in as data points from sentinel 5 if necessary\n",
        "def maskS2clouds(image) :\n",
        "  qa = image.select('QA60');\n",
        "\n",
        "  # Bits 10 and 11 are clouds and cirrus, respectively.\n",
        "  cloudBitMask = 1 << 10\n",
        "  cirrusBitMask = 1 << 11\n",
        "\n",
        "  # Both flags should be set to zero, indicating clear conditions.\n",
        "  mask = qa.bitwiseAnd(cloudBitMask).eq(0).And( \\\n",
        "         qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
        "\n",
        "  return image.updateMask(mask).divide(10000)\n",
        "\n",
        "# High-resolution satellite photograph \n",
        "s2_img = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
        "                  .filterDate(start_date, end_date) \\\n",
        "                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
        "                  .filterBounds(great_britain) \\\n",
        "                  .map(maskS2clouds).median()\n",
        "s2_id = s2_img.getMapId({'bands': ['B4', 'B3', 'B2'], \\\n",
        "                        'min': 0, \\\n",
        "                        'max': 0.3})\n",
        "\n",
        "# Carbon monoxide\n",
        "# Minmax scale is a bit off - recalibrate for Britain (is this still necessary )\n",
        "CO_img = s5_CO.filterDate(start_date, end_date) \\\n",
        "              .filterBounds(great_britain) \\\n",
        "              .select('CO_column_number_density').mean()\n",
        "CO_id = CO_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0, \\\n",
        "    'max': 0.05})\n",
        "\n",
        "# Formaldehyde\n",
        "# Minmax scale is a bit off - recalibrate for Britain\n",
        "HCHO_img = s5_HCHO.filterDate(start_date, end_date) \\\n",
        "                  .filterBounds(great_britain) \\\n",
        "                  .select('tropospheric_HCHO_column_number_density').mean()\n",
        "HCHO_id = HCHO_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0003})\n",
        "\n",
        "# Nitrogen Dioxide\n",
        "NO2_img = s5_NO2.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select('tropospheric_NO2_column_number_density').mean()\n",
        "NO2_id = NO2_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0002})\n",
        "\n",
        "# Ozone\n",
        "O3_img = s5_O3.filterDate(start_date, end_date) \\\n",
        "              .filterBounds(great_britain) \\\n",
        "              .select('O3_column_number_density').mean()\n",
        "O3_id = O3_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.12, \\\n",
        "    'max': 0.15})\n",
        "\n",
        "# Sulphur Dioxide\n",
        "SO2_img = s5_SO2.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select('SO2_column_number_density').mean()\n",
        "SO2_id = SO2_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0005})\n",
        "\n",
        "# Methane\n",
        "CH4_img = s5_CH4.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select('CH4_column_volume_mixing_ratio_dry_air').mean()\n",
        "CH4_id = CH4_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 1750, \\\n",
        "    'max': 1900})"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUZZBD9fhUUw"
      },
      "source": [
        "# For easier iteration down the line. I know I'm not supposed to, but google can't tell me what to do, even if it's a good idea!\n",
        "ghg_imgs = [CO_img, HCHO_img, NO2_img, O3_img, SO2_img, CH4_img]\n",
        "ghg_ids = [CO_id, HCHO_id, NO2_id, O3_id, SO2_id, CH4_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCsfkjOE8y_H"
      },
      "source": [
        "# Visualise data on a Folium map \n",
        "# Attribution has to stay earthengine.google.com, since that's where these maps came from. \n",
        "map = folium.Map(location=[51.5, 0.1], prefer_canvas=True)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=s2_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=CO_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Carbon Monoxide',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=HCHO_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Formaldehyde',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=NO2_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Nitrogen Dioxide',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=O3_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Ozone',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=SO2_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Sulphur Dioxide',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=CH4_id['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Methane',\n",
        "  ).add_to(map)\n",
        "  \n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHIkD4AIJRD9"
      },
      "source": [
        "### Export Data\n",
        "\n",
        "Exports as unmarked .csv tables and GeoTIFF images. \n",
        "\n",
        "##### TODO\n",
        "- Figure out how to get and access location data from fastai. Would writing it out into the filename work? \n",
        "  - If I need to experiment with this KML format that's fine. \n",
        "- Update filepath definitions once training schema is defined \n",
        "- remove random column from csv's \n",
        "- Ensure the hi-res image segmentation is behaving correctly - no overlaps! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56P_LOcgkimq"
      },
      "source": [
        "# All export methods export to the google drive defined above \n",
        "def exportTable(table, scale):\n",
        "  ee.batch.Export.table.toDrive(\n",
        "    collection = sample,\n",
        "    description = str(scale) + 'm_res_csv_export',\n",
        "    folder = str(scale) + \"m\",\n",
        "    fileFormat = \"CSV\"\n",
        "    # geometry = polygon\n",
        "    # selectors = \n",
        "  ).start()\n",
        "\n",
        "# Export one table of the given image, at the scale and dimensions specified.\n",
        "# TODO random column is a placeholder for data sorting - remove when possible! (I could sort by some other parameter, \n",
        "#      but that would influence the sampling)\n",
        "# TODO rewrite export filename prefixes so they aren't all garbage unreadable messes. \n",
        "# image dimensions can be defined at this stage, as can scaling, format\n",
        "# Region can also be defined, allowing iteration here. Minimising the size of the loop is paramount, given the \n",
        "# functional paradigm EE uses.\n",
        "def exportTableFromImage(image, polygon, scale):\n",
        "  sample = image.sampleRegions(\n",
        "      collection = polygon,\n",
        "      geometries = True,\n",
        "      scale = scale\n",
        "  )\n",
        "  exportTable(sample, scale)\n",
        "\n",
        "# Export one GeoTIFF image of the given image, at the scale and dimension specified. \n",
        "# TODO reevaluate image export options - description needs coordinates\n",
        "# maxPixels is just so it lets me export london at 10m/px. Dividing the dataset into 1km squares is the next step. \n",
        "def exportGeotiff(image, polygon, scale):\n",
        "  image_export_options = {\n",
        "    'patchDimensions': [256, 256],\n",
        "    'maxFileSize': 104857600,\n",
        "    'compressed':True \n",
        "  }\n",
        "\n",
        "  ee.batch.Export.image.toDrive(\n",
        "    description = str(scale) + 'm_scale_img',\n",
        "    fileFormat = 'GeoTIFF',\n",
        "    folder = str(scale) + \"m\",\n",
        "    # formatOptions = image_export_options,\n",
        "    image = s2_img,\n",
        "    maxPixels = 10e9,\n",
        "    region = polygon,\n",
        "    scale = scale,\n",
        "    fileDimensions = [100]\n",
        "  ).start()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6D9jluhaqjB"
      },
      "source": [
        "# Create an aggregated FeatureCollection for GHG data, which can then be exported as one csv/kml(?) object\n",
        "\n",
        "aggFC = None \n",
        "skipFirst = True\n",
        "for img in ghg_imgs:\n",
        "  if skipFirst:\n",
        "    skipFirst = False\n",
        "    aggFC = img.sampleRegions(\n",
        "      collection = london,\n",
        "      geometries = True,\n",
        "      scale = 1000\n",
        "      )\n",
        "    continue\n",
        "  \n",
        "  imgFC = img.sampleRegions(\n",
        "      collection = london,\n",
        "      geometries = True,\n",
        "      scale = 1000\n",
        "  )\n",
        "  aggFC = aggFC.merge(imgFC)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT50S7D6kxHM"
      },
      "source": [
        "#exportTable(aggFC, 1000)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGh4FSYhlJyD"
      },
      "source": [
        "# take a sample of the image at the points given and add a random column\n",
        "# TODO combine datasets into one. can tabular recommenders include images? \n",
        "# TOOD Save a small partition in google drive, then work on getting the next via a thread. \n",
        "#      This should also start the training process, then delete the small partition in the drive. \n",
        "\n",
        "# A range of samples to create an iterative downscaler. These could be more evenly spaced. \n",
        "# Reaching an unlikely level of precision below 100m. The convolution map from a 1km scale is very unlikely to have  \n",
        "# enough detail to reconstruct individual roads and houses, but given this volume of data it's not unreasonable to \n",
        "# assume some improvement is still possible.\n",
        "sizes = {1000}#, 500, 100, 50, 10}\n",
        "\n",
        "for scale in sizes:\n",
        "  print(\"this should only be run once, when setting up\")\n",
        "  # exportTableFromImage(CO_img, london, scale)\n",
        "  #exportTableFromImage(HCHO_img, london, scale)\n",
        "  # exportTableFromImage(NO2_img, london, scale)\n",
        "  # exportTableFromImage(O3_img, london, scale)\n",
        "  # exportTableFromImage(SO2_img, london, scale)\n",
        "  # exportTableFromImage(CH4_img, london, scale)\n",
        "\n",
        "pprint(ee.batch.Task.list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imcye1aSmzok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33140c4c-9b8a-4dab-b3a6-f8b150b4086e"
      },
      "source": [
        "pprint(ee.batch.Task.list())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<Task EXPORT_FEATURES: 1000m_res_csv_export (READY)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (CANCELLED)>,\n",
            " <Task EXPORT_IMAGE: 500m_scale_img (CANCELLED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img (CANCELLED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (CANCELLED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 500m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 500m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 50m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 100m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (CANCELLED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (CANCELLED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (CANCELLED)>,\n",
            " <Task EXPORT_IMAGE: 500m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 500m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 500m_scale_img (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (FAILED)>,\n",
            " <Task EXPORT_IMAGE: Image Export (FAILED)>,\n",
            " <Task EXPORT_IMAGE: Image Export (FAILED)>,\n",
            " <Task EXPORT_IMAGE: Image Export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution training data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (FAILED)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbN6AfFGpTjg"
      },
      "source": [
        "sizes = {1000, 500, 10}#, 100, 50, 10}\n",
        "\n",
        "for scale in sizes:\n",
        "  # this should only be run once, when setting up\n",
        "  exportGeotiff(s2_img, london, scale)\n",
        "\n",
        "pprint(ee.batch.Task.list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UR7Gkhq--u0"
      },
      "source": [
        "exportGeotiff(s2_img, great_britain, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWeGM3jenhqM"
      },
      "source": [
        "# Data processing\n",
        "\n",
        "### TODO\n",
        "- ~~Convert GeoTIFF to PNG~~\n",
        "- Reorganise datasets for fast.ai retraining. 2-400 images was sufficient for object recog, maybe double that for top-down sat photos?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojueZsqRrH5P"
      },
      "source": [
        "# converts geotiff to png, using selected bands. There seems to be a limited range of functional bands, including only 3 \n",
        "# being available for a non-transparent image. The bands also display in black-and-white when displayed individually. \n",
        "\n",
        "def geotiffToPng(tif_path):\n",
        "  #TODO remap to ARGB to get more defined brightness data\n",
        "  options_list = [\n",
        "    '-ot Byte',\n",
        "    '-of PNG',\n",
        "    '-b 4',\n",
        "    '-b 3',\n",
        "    '-b 2',\n",
        "    '-scale'\n",
        "  ]\n",
        "  options_string = \" \".join(options_list)\n",
        "\n",
        "  yourpath = os.path.join(drive_path, tif_path)\n",
        "  print(yourpath)\n",
        "  \n",
        "  for root, dirs, files in os.walk(yourpath, topdown=False):\n",
        "    for name in files:\n",
        "      fullpath = os.path.join(root, name)\n",
        "      print(fullpath)\n",
        "      splitpath = os.path.splitext(fullpath)\n",
        "      if splitpath[1].lower() == \".tif\":\n",
        "        path = splitpath[0]\n",
        "        if os.path.isfile(path + \".png\"):\n",
        "          print(\"A png file already exists for %s\" % name)\n",
        "          # Return statement removed while this is under development. I'll clean things up manually till then. \n",
        "          #return\n",
        "        \n",
        "        gdal.Translate(\n",
        "          path + 'band_432.png',\n",
        "          path + '.tif',\n",
        "          options=options_string\n",
        "        )\n",
        "\n",
        "geotiffToPng(\"1000m\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}