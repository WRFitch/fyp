{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fyp_data_import_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DlFd1y1OIXxk",
        "jkq4mhmfIwO_",
        "sTZipoKOJNyg",
        "MglKunWI_fTv",
        "f9l197PkYqGG"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WRFitch/fyp/blob/main/src/fyp_data_import_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBYK4VpU-2tn"
      },
      "source": [
        "# Data Import Pipeline\n",
        "\n",
        "This code is still under construction, and is therefore very very bad in places. \n",
        "\n",
        "### TODO\n",
        "- Import CO2 dataset\n",
        "- Extract unnecessary methods into normal python files and import where necessary. \n",
        "- Remove unnecessary variable changes where necessary - this stacks up all the JSON, making everything harder than it needs to be. \n",
        "  - Actually, it might not\n",
        "- Change unnecessary image imports to feature imports \n",
        "- Figure out a way of iterating through existing images and displaying the area currently covered by my dataset on a map. \n",
        "- Define and import other regions of interest - stick to cities and suburbs for now, since that will have the best health data. Converting this to include rural or rocky areas is an increase in feature set. \n",
        "- Figure out how accurate the image exports are\n",
        "  - Are the points definitely centered on the given coordinates? \n",
        "  - is there a way of standardising lighting? \n",
        "- add file indexing into one CSV with all our existing latlong exports, so we're not constantly querying the filesystem. \n",
        "- list exported files into a CSV \n",
        "- Combine GHG exports into one CSV \n",
        "- Move data processing methods into their own pipeline "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Ua09qtJqgw"
      },
      "source": [
        "## Setup\n",
        "*   Import necessary libraries\n",
        "*   Import fyputil module\n",
        "*   Set up Earth Engine authentication and mount google drive  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1lreKK1bXw"
      },
      "source": [
        "import ee\n",
        "import os\n",
        "import pandas as pd \n",
        "\n",
        "from google.colab import drive\n",
        "# remove this \n",
        "from osgeo import gdal\n",
        "from PIL import Image\n",
        "from pprint import pprint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACtrAfnVnOjt"
      },
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P59lmKIpX8_5"
      },
      "source": [
        "%rm -rf /content/fyp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD9oir7Sh9pX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9adaf86b-edaa-4f95-c035-5a6c9dc1db3b"
      },
      "source": [
        "# Import FYP repo so we can access fyputil common library \n",
        "%cd /content\n",
        "!git clone https://github.com/WRFitch/fyp.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'fyp'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 1053 (delta 46), reused 20 (delta 11), pack-reused 971\u001b[K\n",
            "Receiving objects: 100% (1053/1053), 170.13 MiB | 33.66 MiB/s, done.\n",
            "Resolving deltas: 100% (616/616), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuepO1knXIZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da61f387-46c0-4c0b-937a-1f299ee63969"
      },
      "source": [
        "# Import fyputil library\n",
        "%cd fyp/src/fyputil\n",
        "import constants as c\n",
        "import ee_constants as eec\n",
        "import ee_utils as eeutil\n",
        "import fyp_utils as fyputil\n",
        "%cd /content"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fyp/src/fyputil\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZUKXEfUiEtZ"
      },
      "source": [
        "# Dataset import\n",
        "\n",
        "### Import the following datasets into Google Drive\n",
        "\n",
        "*   [Sentinel-2 Satellite photography](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR)\n",
        "*   [Sentinel-5 Precursor Data](https://developers.google.com/earth-engine/datasets/catalog/sentinel)\n",
        "  *   [Carbon Monoxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CO)\n",
        "  *   [Formaldehyde](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_HCHO)\n",
        "  *   [Nitrogen Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_NO2)\n",
        "  *   [Ozone](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_O3)\n",
        "  *   [Sulphur Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_SO2)\n",
        "  *   [Methane](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CH4)\n",
        "*   [ODIAC Fossil Fuel CO2 Emissions](https://db.cger.nies.go.jp/dataset/ODIAC/DL_odiac2019.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlFd1y1OIXxk"
      },
      "source": [
        "##### Import necessary variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuosygcHIfhO"
      },
      "source": [
        "# Define dataset boundaries for britain and london \n",
        "great_britain = eec.great_britain\n",
        "london = eec.london\n",
        "uxbridge = eec.uxbridge\n",
        "millennium_dome = eec.millennium_dome\n",
        "greenwich = eec.greenwich\n",
        "w_hemisphere = eec.west_hemisphere\n",
        "e_hemisphere = eec.east_hemisphere"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQZlTc5Rf-LC"
      },
      "source": [
        "# TODO move this into a CONSTANTS.py file\n",
        "# Earth engine username, used to import classified image into ee assets folder\n",
        "USERNAME = 'wrfitch'\n",
        "OUTPUT_DIR = USERNAME + \"/out/\"\n",
        "\n",
        "# Define collections for each dataset to be used \n",
        "s2 = eec.s2\n",
        "s5_CO = eec.s5_CO\n",
        "s5_HCHO = eec.s5_HCHO\n",
        "s5_NO2 = eec.s5_NO2\n",
        "s5_O3 = eec.s5_O3\n",
        "s5_SO2 = eec.s5_SO2\n",
        "s5_CH4 = eec.s5_CH4\n",
        "#TODO import CO2 dataset\n",
        "\n",
        "CO_band = c.CO_band\n",
        "HCHO_band = c.HCHO_band\n",
        "NO2_band = c.NO2_band\n",
        "O3_band = c.O3_band\n",
        "SO2_band = c.SO2_band\n",
        "CH4_band = c.CH4_band\n",
        "\n",
        "start_date = eec.start_date\n",
        "end_date = eec.end_date\n",
        "vis_palette = eec.vis_palette\n",
        "\n",
        "drive_path = c.drive_path\n",
        "export_dir = c.export_dir\n",
        "geotiff_dir = c.geotiff_dir"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liIZ0y0JnSjY"
      },
      "source": [
        "# Import datasets \n",
        "\n",
        "# For easier iteration down the line. I know I'm not supposed to, but google \n",
        "# can't tell me what to do, even if it's a good idea!\n",
        "ghg_imgs = eec.ghg_imgs\n",
        "ghg_ids = eec.ghg_ids"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTZipoKOJNyg"
      },
      "source": [
        "### Visualise Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X8Qxxmkg7rN"
      },
      "source": [
        "eec.map "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHIkD4AIJRD9"
      },
      "source": [
        "### Export Data\n",
        "\n",
        "Exports as .csv tables and GeoTIFF images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9l197PkYqGG"
      },
      "source": [
        "#### Exporting CSVs\n",
        "\n",
        "This method of getting the data is very very stupid, but also it does exactly what I need. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyfnXNdtC-Vg"
      },
      "source": [
        "# Only once this is completed can you move forward and get pictures from these spreadsheets.\n",
        "for ghg_img in ghg_imgs:\n",
        "  csv_name = ghg_img.getInfo().get('bands')[0].get('id')\n",
        "  #exportTableFromImage(ghg_img,london, 1000, export_dir, csv_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqzw-XDH5DDO"
      },
      "source": [
        "#### Getting Images From CSV Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuxNYYoumLDO"
      },
      "source": [
        "eeutil.getImgsFromCsv(f\"{c.data_dir}/{c.SO2_band}.csv\", s2_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDu_zz3Vs-Xf"
      },
      "source": [
        "#pprint(ee.batch.Task.list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWeGM3jenhqM"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhaMrmlzNE6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c105cbcd-c4c8-43c2-e3cf-edd0756a5a33"
      },
      "source": [
        "fyputil.geotiffToPng(\"big_geotiff\", f\"{c.export_dir}/png_224\", rm_artifacts=False)\n",
        "fyputil.moveFilesByExtension(\"big_geotiff\", f\"{c.export_dir}/geotiff_224\", \".tif\")\n",
        "fyputil.moveFilesByExtension(\"big_geotiff\", f\"{c.export_dir}/png_224\", \".png\")\n",
        "fyputil.rmConversionArtifacts(\"big_geotiff\", rmTif=False, rmXml=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/big_geotiff\n",
            "/content/drive/MyDrive/big_geotiff\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vkByB6_ZhwH"
      },
      "source": [
        "fyputil.geotiffToPng(f\"{c.export_dir}/geotiff_224\", f\"{c.export_dir}/png_224\", rm_artifacts=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWkxiQtyZxT9"
      },
      "source": [
        "fyputil.moveFilesByExtension(f\"{c.export_dir}/geotiff_224\", f\"{c.export_dir}/png_224\", \".png\")\n",
        "fyputil.rmConversionArtifacts(f\"{c.export_dir}/geotiff_224\", rmTif=False, rmXml=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkr6RzUt_atC"
      },
      "source": [
        "# Cleaning up if things go a bit wrong\n",
        "fyputil.moveFilesByExtension(f\"{export_dir}/geotiff\", f\"{export_dir}/png\", \".png\")\n",
        "fyputil.rmConversionArtifacts(f\"{export_dir}/geotiff\", rmTif=False, rmXml=True)\n",
        "fyputil.moveFilesByExtension(geotiff_dir, f\"{export_dir}/geotiff\", \".tif\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGUu7dyWn9jh"
      },
      "source": [
        "# TODO add these all into one csv file as part of import pipeline. \n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/{CO_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/{HCHO_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/{NO2_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/{O3_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/{SO2_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/{CH4_band}.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyNxAy0AsQu2"
      },
      "source": [
        "# Parse CSVs into pandas dataframes\n",
        "# TODO rewrite so we aren't deleting columns directly - do it properly! Incorporate these into one csv export in the \n",
        "#      output pipeline \n",
        "path = c.data_dir\n",
        "co_df = pd.read_csv(f\"{path}/{CO_band}.csv\")\n",
        "del co_df[\".geo\"]\n",
        "hcho_df = pd.read_csv(f\"{path}/{HCHO_band}.csv\")\n",
        "del hcho_df[\".geo\"]\n",
        "no2_df = pd.read_csv(f\"{path}/{NO2_band}.csv\")\n",
        "del no2_df[\".geo\"]\n",
        "o3_df = pd.read_csv(f\"{path}/{O3_band}.csv\")\n",
        "del o3_df[\".geo\"]\n",
        "so2_df = pd.read_csv(f\"{path}/{SO2_band}.csv\")\n",
        "del so2_df[\".geo\"]\n",
        "ch4_df = pd.read_csv(f\"{path}/{CH4_band}.csv\")\n",
        "del ch4_df[\".geo\"]\n",
        "\n",
        "# Incorporate individual csvs into one ghg dataframe. Badly. \n",
        "# TODO fix this so we aren't repeating the same thing over and over\n",
        "mrg_params = ['longitude', 'latitude', 'system:index']\n",
        "# somehow this means \"intersect\". We're taking the intersect so we know we have common values. \n",
        "mrg_type = 'inner'\n",
        "\n",
        "intersect = pd.merge(so2_df, ch4_df, how=mrg_type, on=mrg_params)\n",
        "intersect = pd.merge(intersect, co_df, how=mrg_type, on=mrg_params)\n",
        "intersect = pd.merge(intersect, hcho_df, how=mrg_type, on=mrg_params)\n",
        "intersect = pd.merge(intersect, no2_df, how=mrg_type, on=mrg_params)\n",
        "intersect = pd.merge(intersect, o3_df, how=mrg_type, on=mrg_params)\n",
        "\n",
        "print(intersect.shape)\n",
        "intersect.iloc[0:4] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbo-sIH-yDo9"
      },
      "source": [
        "raw_ghg_df = intersect.copy()\n",
        "\n",
        "for index, row in intersect.iterrows():\n",
        "  coords = (row.longitude, row.latitude)\n",
        "  #print(coords)\n",
        "  filepath = f\"{c.data_dir}/png_224/{coords[0]}_{coords[1]}.png\"\n",
        "  if not os.path.isfile(filepath):\n",
        "    print(f\"dropping {filepath} from row {index}\")\n",
        "    # TODO implement this in a way that doesn't suck. \n",
        "    # At least unify implementation in a method or management pipeline \n",
        "    raw_ghg_df = raw_ghg_df.drop(index=index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCdxrBST4xSr"
      },
      "source": [
        "print(intersect.shape)\n",
        "print(raw_ghg_df.shape)\n",
        "raw_ghg_df.iloc[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8VCVIjjyF18"
      },
      "source": [
        "raw_ghg_df.to_csv(f\"{c.data_dir}/ghgs.csv\")"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}