{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fyp_data_import_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DlFd1y1OIXxk",
        "jkq4mhmfIwO_",
        "sTZipoKOJNyg",
        "MglKunWI_fTv",
        "f9l197PkYqGG"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WRFitch/fyp/blob/main/src/fyp_data_import_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBYK4VpU-2tn"
      },
      "source": [
        "# Data Import Pipeline\n",
        "\n",
        "This is the data import pipeline used to import the greenhouse gas and satellite photography dataset from Google Earth Engine, as well as processing and saving it into usable CSV files. \n",
        "\n",
        "### TODO\n",
        "- Import CO2 dataset\n",
        "- Figure out a way of iterating through existing images and displaying the area currently covered by my dataset on a map. \n",
        "- Define and import other regions of interest - stick to cities and suburbs for now, since that will have the best health data. Converting this to include rural or rocky areas is an increase in feature set. \n",
        "- list exported files into a CSV "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Ua09qtJqgw"
      },
      "source": [
        "## Setup\n",
        "*   Import necessary libraries\n",
        "*   Import fyputil module\n",
        "*   Set up Earth Engine authentication and mount google drive  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1lreKK1bXw"
      },
      "source": [
        "import os\n",
        "import pandas as pd \n",
        "\n",
        "from google.colab import drive\n",
        "from osgeo import gdal\n",
        "from pprint import pprint\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P59lmKIpX8_5"
      },
      "source": [
        "%rm -rf /content/fyp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLEqwoJh_uVl"
      },
      "source": [
        "# Import FYP repo so we can access fyputil common library \n",
        "%cd /content\n",
        "!git clone https://github.com/WRFitch/fyp.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD9oir7Sh9pX"
      },
      "source": [
        "%cd /content/fyp/src/fyputil\n",
        "\n",
        "# Import fyputil library\n",
        "import constants as c\n",
        "import fyp_utils as fyputil\n",
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjbTj36gVOZN"
      },
      "source": [
        "### Earth Engine Only! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKW0R9lc_lzw"
      },
      "source": [
        "import ee\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "\n",
        "%cd /content/fyp/src/fyputil\n",
        "import ee_constants as eec\n",
        "import ee_utils as eeutil\n",
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZUKXEfUiEtZ"
      },
      "source": [
        "# Dataset import\n",
        "\n",
        "### Import the following datasets into Google Drive\n",
        "\n",
        "*   [Sentinel-2 Satellite photography](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR)\n",
        "*   [Sentinel-5 Precursor Data](https://developers.google.com/earth-engine/datasets/catalog/sentinel)\n",
        "  *   [Carbon Monoxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CO)\n",
        "  *   [Formaldehyde](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_HCHO)\n",
        "  *   [Nitrogen Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_NO2)\n",
        "  *   [Ozone](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_O3)\n",
        "  *   [Sulphur Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_SO2)\n",
        "  *   [Methane](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CH4)\n",
        "*   [ODIAC Fossil Fuel CO2 Emissions](https://db.cger.nies.go.jp/dataset/ODIAC/DL_odiac2019.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTZipoKOJNyg"
      },
      "source": [
        "### Visualise Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X8Qxxmkg7rN"
      },
      "source": [
        "eec.map "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHIkD4AIJRD9"
      },
      "source": [
        "## Import Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9l197PkYqGG"
      },
      "source": [
        "#### Importing CSVs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyfnXNdtC-Vg"
      },
      "source": [
        "# Only once this is completed can you move forward and get pictures from these \n",
        "# spreadsheets.\n",
        "for ghg_img in eec.ghg_imgs:\n",
        "  csv_name = ghg_img.getInfo().get('bands')[0].get('id')\n",
        "  print(csv_name)\n",
        "  #eeutil.exportTableFromImage(ghg_img, eec.south_east, 1000, c.export_dir, csv_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqzw-XDH5DDO"
      },
      "source": [
        "#### Getting Images From CSV Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuxNYYoumLDO"
      },
      "source": [
        "eeutil.getImgsFromCsv(f\"{c.data_dir}/{c.SO2_band}.csv\", eec.s2_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDu_zz3Vs-Xf"
      },
      "source": [
        "#pprint(ee.batch.Task.list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWeGM3jenhqM"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhaMrmlzNE6-"
      },
      "source": [
        "# Split these lines out and run individually if you run into trouble\n",
        "fyputil.geotiffToPng(\"big_geotiff\", f\"{c.export_dir}/png_224\", rm_artifacts=False)\n",
        "fyputil.moveFilesByExtension(\"big_geotiff\", f\"{c.export_dir}/geotiff_224\", \".tif\")\n",
        "fyputil.moveFilesByExtension(\"big_geotiff\", f\"{c.export_dir}/png_224\", \".png\")\n",
        "fyputil.rmConversionArtifacts(\"big_geotiff\", rmTif=False, rmXml=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJjwrwycH0cU"
      },
      "source": [
        "%ls {c.big_png_dir} | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vkByB6_ZhwH"
      },
      "source": [
        "fyputil.geotiffToPng(f\"{c.export_dir}/geotiff_224\", f\"{c.export_dir}/png_224\", rm_artifacts=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWkxiQtyZxT9"
      },
      "source": [
        "fyputil.moveFilesByExtension(f\"{c.export_dir}/geotiff_224\", f\"{c.export_dir}/png_224\", \".png\")\n",
        "fyputil.rmConversionArtifacts(f\"{c.export_dir}/geotiff_224\", rmTif=False, rmXml=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU0Z3mwYD5_p"
      },
      "source": [
        "###Incorporate all CSVs into one\n",
        "\n",
        "This could be more efficient, but it's only processed once per dataset and is far from the slowest portion of this codebase. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGUu7dyWn9jh"
      },
      "source": [
        "# Parse .geo column into longitude and latitude columns \n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/se-{c.CO_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/se-{c.HCHO_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/se-{c.NO2_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/se-{c.O3_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/se-{c.SO2_band}.csv\")\n",
        "fyputil.parseCsvCoords(f\"{c.data_dir}/se-{c.CH4_band}.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyNxAy0AsQu2"
      },
      "source": [
        "# Parse CSVs into pandas dataframes\n",
        "# TODO rewrite so we aren't deleting columns directly - do it properly! \n",
        "#      Incorporate these into one csv export in the output pipeline \n",
        "co_df = pd.read_csv(f\"{c.data_dir}/se-{c.CO_band}.csv\")\n",
        "del co_df[\".geo\"]\n",
        "hcho_df = pd.read_csv(f\"{c.data_dir}/se-{c.HCHO_band}.csv\")\n",
        "del hcho_df[\".geo\"]\n",
        "no2_df = pd.read_csv(f\"{c.data_dir}/se-{c.NO2_band}.csv\")\n",
        "del no2_df[\".geo\"]\n",
        "o3_df = pd.read_csv(f\"{c.data_dir}/se-{c.O3_band}.csv\")\n",
        "del o3_df[\".geo\"]\n",
        "so2_df = pd.read_csv(f\"{c.data_dir}/se-{c.SO2_band}.csv\")\n",
        "del so2_df[\".geo\"]\n",
        "ch4_df = pd.read_csv(f\"{c.data_dir}/se-{c.CH4_band}.csv\")\n",
        "del ch4_df[\".geo\"]\n",
        "\n",
        "# Incorporate individual csvs into one ghg dataframe.\n",
        "# TODO fix this so we aren't repeating the same thing over and over\n",
        "mrg_params = ['longitude', 'latitude']\n",
        "# somehow this means \"intersect\". We're taking the intersect so we know we have \n",
        "#common values. \n",
        "mrg_type = 'inner'\n",
        "\n",
        "intersect = pd.merge(so2_df, ch4_df, how=mrg_type, on=mrg_params)\n",
        "intersect = pd.merge(intersect, co_df, how=mrg_type, on=mrg_params)\n",
        "intersect = pd.merge(intersect, hcho_df, how=mrg_type, on=mrg_params)\n",
        "intersect = pd.merge(intersect, no2_df, how=mrg_type, on=mrg_params)\n",
        "intersect = pd.merge(intersect, o3_df, how=mrg_type, on=mrg_params)\n",
        "\n",
        "print(intersect.shape)\n",
        "intersect.iloc[0:4] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_fPSymy0bJV"
      },
      "source": [
        "# Hacky again, but it'll do for now\n",
        "del intersect[\"system:index_x\"]\n",
        "del intersect[\"system:index_y\"]\n",
        "intersect.iloc[0:4] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbo-sIH-yDo9"
      },
      "source": [
        "# Ensure we're only using imported images in the ghg csv. \n",
        "# NOTE influencing data in this way isn't worthwhile, since it affects the \n",
        "#      dataset needlessly and the rest of the codebase is built to work around \n",
        "#      this. Avoid running this block. \n",
        "raw_ghg_df = intersect.copy()\n",
        "\n",
        "no_file_idx = []\n",
        "for index, row in intersect.iterrows():\n",
        "  coords = (row.longitude, row.latitude)\n",
        "  #print(coords)\n",
        "  filepath = f\"{c.data_dir}/png_224/{coords[0]}_{coords[1]}.png\"\n",
        "  if not os.path.isfile(filepath):\n",
        "    print(f\"dropping {filepath} from row {index}\")\n",
        "    #raw_ghg_df = raw_ghg_df.drop(index=index)\n",
        "    no_file_idx.append(index)\n",
        "\n",
        "raw_ghg_df = raw_ghg_df.drop(no_file_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCdxrBST4xSr"
      },
      "source": [
        "print(intersect.shape)\n",
        "print(raw_ghg_df.shape)\n",
        "raw_ghg_df.iloc[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8VCVIjjyF18"
      },
      "source": [
        "# Export to csv. Commented out to ensure it's not run accidentally. \n",
        "#intersect.to_csv(f\"{c.data_dir}/se-ghgs.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}