{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fyp_data_import_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WRFitch/fyp/blob/main/src/fyp_data_import_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBYK4VpU-2tn"
      },
      "source": [
        "# Data Import Pipeline\n",
        "It's a pipeline that imports the data I need. \n",
        "\n",
        "### TODO\n",
        "- Implement image splitting into 100px squares\n",
        "- Import CO2 dataset\n",
        "- Implement a normal coding standard for variable names, method names, etc\n",
        "- Extract unnecessary methods into normal python files and import where necessary. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Ua09qtJqgw"
      },
      "source": [
        "## Setup\n",
        "*   Import necessary libraries\n",
        "*   Set up Earth Engine authentication and mount google drive  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT1lreKK1bXw"
      },
      "source": [
        "import ee\n",
        "import folium\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "from osgeo import gdal\n",
        "from PIL import Image\n",
        "from pprint import pprint"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACtrAfnVnOjt"
      },
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLUtYn4SndcT"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "print(folium.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZUKXEfUiEtZ"
      },
      "source": [
        "# Dataset import\n",
        "\n",
        "### Import the following datasets into Google Drive\n",
        "\n",
        "*   [Sentinel-2 Satellite photography](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR)\n",
        "*   [Sentinel-5 Precursor Data](https://developers.google.com/earth-engine/datasets/catalog/sentinel)\n",
        "  *   [Carbon Monoxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CO)\n",
        "  *   [Formaldehyde](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_HCHO)\n",
        "  *   [Nitrogen Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_NO2)\n",
        "  *   [Ozone](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_O3)\n",
        "  *   [Sulphur Dioxide](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_SO2)\n",
        "  *   [Methane](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S5P_OFFL_L3_CH4)\n",
        "*   [ODIAC Fossil Fuel CO2 Emissions](https://db.cger.nies.go.jp/dataset/ODIAC/DL_odiac2019.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQZlTc5Rf-LC"
      },
      "source": [
        "# Earth engine username, used to import classified image into ee assets folder\n",
        "USERNAME = 'wrfitch'\n",
        "OUTPUT_DIR = USERNAME + \"/out/\"\n",
        "\n",
        "# Define image collections for each dataset to be used \n",
        "s2 = ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
        "s5_CO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\n",
        "s5_HCHO = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_HCHO\")\n",
        "s5_NO2 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_NO2\")\n",
        "s5_O3 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_O3\")\n",
        "s5_SO2 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_SO2\")\n",
        "s5_CH4 = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CH4\")\n",
        "#TODO import CO2 dataset\n",
        "\n",
        "CO_band = 'CO_column_number_density'\n",
        "HCHO_band = 'tropospheric_HCHO_column_number_density'\n",
        "NO2_band = 'tropospheric_NO2_column_number_density'\n",
        "O3_band = 'O3_column_number_density'\n",
        "SO2_band = 'SO2_column_number_density'\n",
        "CH4_band = 'CH4_column_volume_mixing_ratio_dry_air'\n",
        "\n",
        "# Define dataset boundaries for britain and london \n",
        "# TODO work out polygon segmentation algo - there's probably a clever algorithm for this, but I could also just iterate \n",
        "#      through simple squares that fit my bandwidth and storage constraints. I run out of memory when using the gbr \n",
        "#      polygon anyway, so an iterative approach is necessary. \n",
        "# TODO evaluate projections as a source of potential image distortion \n",
        "great_britain = ee.Geometry.Polygon(\n",
        "        [[[-1.836112801004015, 59.808076330562756],\n",
        "          [-8.779472176004015, 58.82140293049428],\n",
        "          [-7.988456551004015, 55.71069203454839],\n",
        "          [-11.196464363504015, 54.42753859549109],\n",
        "          [-11.328300301004015, 50.967746003015044],\n",
        "          [-9.526542488504015, 50.77361752815123],\n",
        "          [-6.274589363504015, 51.81776248652293],\n",
        "          [-5.395683113504015, 51.21615275310099],\n",
        "          [-6.582206551004015, 49.56332371186494],\n",
        "          [-3.110526863504015, 49.904165426606255],\n",
        "          [1.240059073995985, 50.80139967619036],\n",
        "          [2.426582511495985, 52.33095407387208],\n",
        "          [1.767402823995985, 53.4183511305661],\n",
        "          [0.5369340739959849, 53.44453305344514],\n",
        "          [-1.616386238504015, 56.32474216074427],\n",
        "          [-0.7814253010040151, 57.805828290000164]]])\n",
        "\n",
        "london = ee.Geometry.Polygon(\n",
        "        [[[-1.0666833726431624, 51.89360084338857],\n",
        "          [-0.9321008531119124, 51.38908166135181],\n",
        "          [-0.18503054061191238, 51.08470683562287],\n",
        "          [0.4741491468881076, 51.193274483099074],\n",
        "          [0.9822668226693576, 51.60282356474035],\n",
        "          [0.2269567640756076, 52.071221592742454]]])\n",
        "\n",
        "uxbridge = ee.Geometry.Rectangle(\n",
        "        [[-0.5585304622116949, 51.577993458567235],\n",
        "          [-0.3951088313523199, 51.51009512268249]])\n",
        "\n",
        "#define crs object so we can use the same coordinate reference system on everything. \n",
        "#crs = ee.Projection.\n",
        "\n",
        "# Could the start and end dates be shifted or focused on one area, so emissions can be monitored across the seasons? \n",
        "# would that even be useful? \n",
        "start_date = '2020-01-01'\n",
        "end_date = '2020-12-31'\n",
        "vis_palette = ['black', 'blue', 'purple', 'cyan', 'green', 'yellow', 'red']\n",
        "\n",
        "drive_path = \"/content/drive/MyDrive/\""
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTZipoKOJNyg"
      },
      "source": [
        "### Visualise Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liIZ0y0JnSjY"
      },
      "source": [
        "# Import datasets \n",
        "# TODO analyse whether these min/max values are valid, recalibrate for highest variance where necessary. Separate values\n",
        "#      may be necessary for different samples - for example, the perfect calibration for the UK won't work on the world. \n",
        "# TODO analyse whether it makes sense to analyse these on a highly localised level\n",
        "\n",
        "# pre-filter to remove clouds - we can add them back in as data points from sentinel 5 if necessary\n",
        "def maskS2clouds(image) :\n",
        "  qa = image.select('QA60');\n",
        "\n",
        "  # Bits 10 and 11 are clouds and cirrus, respectively.\n",
        "  cloud_bitmask = 1 << 10\n",
        "  cirrus_bitmask = 1 << 11\n",
        "\n",
        "  # Both flags should be set to zero, indicating clear conditions.\n",
        "  mask = qa.bitwiseAnd(cloud_bitmask).eq(0).And( \\\n",
        "         qa.bitwiseAnd(cirrus_bitmask).eq(0))\n",
        "\n",
        "  return image.updateMask(mask).divide(10000)\n",
        "\n",
        "# High-resolution satellite photograph \n",
        "s2_img = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
        "                  .filterDate(start_date, end_date) \\\n",
        "                  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
        "                  .filterBounds(great_britain) \\\n",
        "                  .map(maskS2clouds).median()\n",
        "s2_id = s2_img.getMapId({'bands': ['B4', 'B3', 'B2'], \\\n",
        "                        'min': 0, \\\n",
        "                        'max': 0.3})\n",
        "\n",
        "# Carbon monoxide\n",
        "# Minmax scale is a bit off - recalibrate for Britain \n",
        "CO_img = s5_CO.filterDate(start_date, end_date) \\\n",
        "              .filterBounds(great_britain) \\\n",
        "              .select(CO_band).mean()\n",
        "CO_id = CO_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0, \\\n",
        "    'max': 0.05})\n",
        "\n",
        "# Formaldehyde\n",
        "# Minmax scale is a bit off - recalibrate for Britain\n",
        "HCHO_img = s5_HCHO.filterDate(start_date, end_date) \\\n",
        "                  .filterBounds(great_britain) \\\n",
        "                  .select(HCHO_band).mean()\n",
        "HCHO_id = HCHO_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0003})\n",
        "\n",
        "# Nitrogen Dioxide\n",
        "NO2_img = s5_NO2.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select(NO2_band).mean()\n",
        "NO2_id = NO2_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0002})\n",
        "\n",
        "# Ozone\n",
        "O3_img = s5_O3.filterDate(start_date, end_date) \\\n",
        "              .filterBounds(great_britain) \\\n",
        "              .select(O3_band).mean()\n",
        "O3_id = O3_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.12, \\\n",
        "    'max': 0.15})\n",
        "\n",
        "# Sulphur Dioxide\n",
        "SO2_img = s5_SO2.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select(SO2_band).mean()\n",
        "SO2_id = SO2_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 0.0, \\\n",
        "    'max': 0.0005})\n",
        "\n",
        "# Methane\n",
        "CH4_img = s5_CH4.filterDate(start_date, end_date) \\\n",
        "                .filterBounds(great_britain) \\\n",
        "                .select(CH4_band).mean()\n",
        "CH4_id = CH4_img.getMapId( \\\n",
        "    {'palette': vis_palette, \\\n",
        "    'min': 1750, \\\n",
        "    'max': 1900})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUZZBD9fhUUw"
      },
      "source": [
        "# For easier iteration down the line. I know I'm not supposed to, but google can't tell me what to do, even if it's a good idea!\n",
        "ghg_imgs = [CO_img, HCHO_img, NO2_img, O3_img, SO2_img, CH4_img]\n",
        "ghg_ids = [CO_id, HCHO_id, NO2_id, O3_id, SO2_id, CH4_id]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCsfkjOE8y_H"
      },
      "source": [
        "# Visualise data on a Folium map \n",
        "# Attribution has to stay earthengine.google.com, since that's where these maps came from. \n",
        "map = folium.Map(\n",
        "    location = [51.5, 0.1], \n",
        "    prefer_canvas = True)\n",
        "\n",
        "layerOpacity = 0.5\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = s2_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'satellite photography median composite '\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = CO_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'Carbon Monoxide',\n",
        "    opacity = layerOpacity\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = HCHO_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'Formaldehyde',\n",
        "    opacity = layerOpacity\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = NO2_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'Nitrogen Dioxide',\n",
        "    opacity = layerOpacity\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = O3_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'Ozone',\n",
        "    opacity = layerOpacity\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = SO2_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'Sulphur Dioxide',\n",
        "    opacity = layerOpacity\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles = CH4_id['tile_fetcher'].url_format,\n",
        "    attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay = True,\n",
        "    name = 'Methane',\n",
        "    opacity = layerOpacity\n",
        "  ).add_to(map)\n",
        "  \n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHIkD4AIJRD9"
      },
      "source": [
        "### Export Data\n",
        "\n",
        "Exports as unmarked .csv tables and GeoTIFF images. \n",
        "\n",
        "##### TODO\n",
        "- Figure out how to get and access location data from fastai. Would writing it out into the filename work? \n",
        "  - If I need to experiment with this KML format that's fine. \n",
        "- Update filepath definitions once training schema is defined \n",
        "- Ensure the hi-res image segmentation is behaving correctly - no overlaps! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56P_LOcgkimq"
      },
      "source": [
        "# All export methods export to the google drive defined above \n",
        "# TODO A lot of these methods reuse scaling. Investigate whether this is necessary and remove where possible. \n",
        "# TODO rewrite export filename prefixes so they aren't garbage\n",
        "def exportTable(table, scale):\n",
        "  ee.batch.Export.table.toDrive(\n",
        "    collection = table,\n",
        "    description = str(scale) + 'm_res_csv_export',\n",
        "    folder = str(scale) + \"m\",\n",
        "    fileFormat = \"CSV\"\n",
        "    # geometry = polygon\n",
        "    # selectors = \n",
        "  ).start()\n",
        "\n",
        "# Export one table of the given image, at the scale and dimensions specified.\n",
        "def exportTableFromImage(image, polygon, scale):\n",
        "  exportTable(sample(image, polygon, scale), scale)\n",
        "\n",
        "# Export one GeoTIFF image of the given image, at the scale and dimension specified. \n",
        "# TODO reevaluate image export options - description needs coordinates\n",
        "# maxPixels is just so it lets me export london at 10m/px. Dividing the dataset into 1km squares is the next step. \n",
        "def exportGeotiff(image, polygon, scale):\n",
        "  ee.batch.Export.image.toDrive(\n",
        "    description = str(scale) + 'm_scale_img',\n",
        "    fileFormat = 'GeoTIFF',\n",
        "    folder = str(scale) + \"m\",\n",
        "    # formatOptions = image_export_options,\n",
        "    image = image,\n",
        "    maxPixels = 10e9,\n",
        "    region = polygon,\n",
        "    scale = scale\n",
        "  ).start()\n",
        "  \n",
        "# Does this really need to exist?\n",
        "def sample(img, region, scale):\n",
        "  return img.sampleRegions(\n",
        "      collection = region,\n",
        "      geometries = True,\n",
        "      scale = scale\n",
        "  )\n",
        "\n",
        "#Export the given image as a list of geotiffs in the given polygon, broken into 100px^2 1km images. \n",
        "def exportSqKmGeotiffList(image, polygon, scale):\n",
        "  #Get image corner coordinates so the image can be iterated through. \n",
        "\n",
        "  ee.batch.Export.image.toDrive(\n",
        "      description = str(scale) + \"m_scale_img_list\",\n",
        "      fileFormat = \"GeoTIFF\",\n",
        "      folder = str(scale) + \"m/sqkm_png_exports\",\n",
        "      image = image,\n",
        "      maxPixels = 10e9,\n",
        "      region = polygon,\n",
        "      scale = scale\n",
        "      # shardSize = 100, # shardSize is deprecated on cloud API. This is not documented. \n",
        "      #fileDimensions = [25600, 25600]\n",
        "  ).start()"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGh4FSYhlJyD"
      },
      "source": [
        "# take a sample of the image at the points given\n",
        "# TOOD Save a small partition in google drive, then work on getting the next via a thread. \n",
        "#      This should also start the training process, then delete the small partition in the drive. \n",
        "sizes = {1000}#, 500, 100, 50, 10}\n",
        "\n",
        "for scale in sizes:\n",
        "  print(\"this should only be run once, when setting up\")\n",
        "  \"\"\"\n",
        "  exportTableFromImage(CO_img, london, scale)\n",
        "  exportTableFromImage(HCHO_img, london, scale)\n",
        "  exportTableFromImage(NO2_img, london, scale)\n",
        "  exportTableFromImage(O3_img, london, scale)\n",
        "  exportTableFromImage(SO2_img, london, scale)\n",
        "  exportTableFromImage(CH4_img, london, scale)\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imcye1aSmzok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8a437b-8880-4333-f090-956a8781903c"
      },
      "source": [
        "pprint(ee.batch.Task.list())"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<Task EXPORT_IMAGE: 10m_scale_img_list (READY)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img (READY)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img_list (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img_list (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img_list (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img_list (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img_list (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img_list (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img (CANCELLED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img_list (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img_list (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img_list (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (FAILED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (FAILED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (FAILED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: 1000m_res_csv_export (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (CANCELLED)>,\n",
            " <Task EXPORT_IMAGE: 500m_scale_img (CANCELLED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img (CANCELLED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (CANCELLED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 500m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 500m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 10m_scale_img (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 50m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 100m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (CANCELLED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (CANCELLED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (CANCELLED)>,\n",
            " <Task EXPORT_IMAGE: 500m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 500m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (COMPLETED)>,\n",
            " <Task EXPORT_IMAGE: 500m_scale_img (FAILED)>,\n",
            " <Task EXPORT_IMAGE: 1000m_scale_img (FAILED)>,\n",
            " <Task EXPORT_IMAGE: Image Export (FAILED)>,\n",
            " <Task EXPORT_IMAGE: Image Export (FAILED)>,\n",
            " <Task EXPORT_IMAGE: Image Export (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 500m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting data at 1000m resolution (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution testing data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution testing data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting low-resolution training data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (COMPLETED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (FAILED)>,\n",
            " <Task EXPORT_FEATURES: exporting high-resolution training data (FAILED)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbN6AfFGpTjg"
      },
      "source": [
        "sizes = {10}#, 100, 50, 10} 1000, 500, \n",
        "\n",
        "for scale in sizes:\n",
        "  # this should only be run once, when setting up\n",
        "  exportGeotiff(s2_img, london, scale)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyVdAmac7tYR"
      },
      "source": [
        "#exportGeotiff(s2_img, uxbridge, 10)\n",
        "exportSqKmGeotiffList(s2_img, uxbridge, 10)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UR7Gkhq--u0"
      },
      "source": [
        "#exportGeotiff(s2_img, great_britain, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWeGM3jenhqM"
      },
      "source": [
        "# Data processing\n",
        "\n",
        "### TODO\n",
        "- ~~Convert GeoTIFF to PNG~~\n",
        "- Reorganise datasets for fast.ai retraining. 2-400 images was sufficient for object recog, maybe double that for top-down sat photos?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojueZsqRrH5P"
      },
      "source": [
        "# converts geotiff to png, using selected bands. There seems to be a limited range of functional bands, including only 3 \n",
        "# being available for a non-transparent image. The bands also display in black-and-white when displayed individually. \n",
        "\n",
        "def geotiffToPng(tif_path):\n",
        "  #TODO remap to ARGB to get more defined brightness data\n",
        "  options_list = [\n",
        "    '-ot Byte',\n",
        "    '-of PNG',\n",
        "    '-b 4',\n",
        "    '-b 3',\n",
        "    '-b 2',\n",
        "    '-scale'\n",
        "  ]\n",
        "  options_string = \" \".join(options_list)\n",
        "\n",
        "  yourpath = os.path.join(drive_path, tif_path)\n",
        "  print(yourpath)\n",
        "  \n",
        "  for root, dirs, files in os.walk(yourpath, topdown=False):\n",
        "    for name in files:\n",
        "      fullpath = os.path.join(root, name)\n",
        "      print(fullpath)\n",
        "      splitpath = os.path.splitext(fullpath)\n",
        "      if splitpath[1].lower() == \".tif\":\n",
        "        path = splitpath[0]\n",
        "        print(path)\n",
        "        if os.path.isfile(path + \".png\"):\n",
        "          print(\"A png file already exists for %s\" % name)\n",
        "          # Return statement removed while this is under development. I'll clean things up manually till then. \n",
        "          #return\n",
        "        \n",
        "        gdal.Translate(\n",
        "          path + '.png',\n",
        "          path + '.tif',\n",
        "          options=options_string\n",
        "        )"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh5sPy3Gn3gN",
        "outputId": "022a142c-a7cc-4d32-a6e2-6fdcd6c2d8ae"
      },
      "source": [
        "#geotiffToPng(\"10m/sqkm_png_exports\")\n",
        "geotiffToPng(\"10m sqkm_png_exports\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/10m sqkm_png_exports\n",
            "/content/drive/MyDrive/10m sqkm_png_exports/10m_scale_img_list.tif\n",
            "/content/drive/MyDrive/10m sqkm_png_exports/10m_scale_img_list\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}